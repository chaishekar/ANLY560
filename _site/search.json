[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "What is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed."
  },
  {
    "objectID": "dv.html#bitcoin-plot-using-plotly",
    "href": "dv.html#bitcoin-plot-using-plotly",
    "title": "Data Visualization",
    "section": "Bitcoin plot using plotly",
    "text": "Bitcoin plot using plotly\nOR you can obtain a single stock price\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n\n\n[1] \"2021-09-15\"\n\n\n[1] \"2023-02-16\"\n\n\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n           rownames.bitc.\n2021-09-15     2021-09-15\n2021-09-16     2021-09-16\n2021-09-17     2021-09-17\n2021-09-20     2021-09-20\n2021-09-21     2021-09-21\n2021-09-22     2021-09-22\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n                 date\n2021-09-15 2021-09-15\n2021-09-16 2021-09-16\n2021-09-17 2021-09-17\n2021-09-20 2021-09-20\n2021-09-21 2021-09-21\n2021-09-22 2021-09-22\n\n\n'data.frame':   359 obs. of  7 variables:\n $ BTC.Open    : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.High    : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Low     : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.Close   : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Volume  : num  102 877 502 110 464 ...\n $ BTC.Adjusted: num  99.7 99.6 99.5 99.6 99.6 ...\n $ date        : Date, format: \"2021-09-15\" \"2021-09-16\" ...\n\n\n\n\n\n\n\n\n\nThe stock price of Bitcoin diminishes with time."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The stock market is a complex, ever-changing system where people can buy and sell shares of companies in order to make money. Several things, like economic growth, inflation, interest rates, government policies, and world events, can have an effect on the stock market. So, figuring out the trends and movements of the stock market can be hard, but it’s important for investors to do so so they can make good decisions and reduce the risk of losing money.\nThere are two main types of analysis that are used to make stock market predictions: fundamental analysis and technical analysis. Fundamental analysis looks at things like the price-to-earnings ratio to figure out how well a company or the economy as a whole is doing. Top-down fundamental analysis starts with the economy as a whole and predicts how it will affect each stock. Bottom-up fundamental analysis, on the other hand, looks at a company’s financial data to figure out how well it is doing. On the other hand, technical analysis looks for patterns in stock charts and uses past data to predict how prices will move in the future.\n\nInvestors can use tools like time series analysis, trend charts, and seasonal changes to make predictions about the stock market. These tools can help investors find patterns and trends in the market so they can buy or sell shares with confidence. When making stock market predictions, it is important to think about the needs and goals of each investor.\nEven though the stock market can be a great way to make money, many people don’t invest in it because they think it’s too risky or hard to understand. Because of this, it is important to teach investors about the stock market and its possible benefits and risks. Many news outlets, like CNBC, give regular updates and predictions about the stock market. This makes it easier for both new and experienced investors to understand.\nIn conclusion, predicting trends and movements in the stock market is hard but important for investors who want to make the most money and take the least amount of risk. Investors can find patterns and trends in the market and make smart decisions about whether to buy or sell shares by using different tools and methods. Educating and informing investors about the stock market and its possible benefits and risks can also make it easier for people to invest in and less scary.\nQUESTIONS:\n\nHow can time series analysis help investors make informed decisions about buying or selling stocks based on macroeconomic factors?\nWhat role do historical trends in macroeconomic indicators play in predicting stock market performance, and how can this information be used to guide investment strategies?\nWhat are some practical methods for incorporating macroeconomic factors into fundamental and technical analyses of individual stocks or sectors?\nHow can macroeconomic indicators be used to identify potential risks or opportunities in the stock market, and what strategies can be employed to manage these risks or capitalize on these opportunities?\nHow can investors stay informed about key macroeconomic factors that are likely to impact the stock market, and what resources are available for tracking these factors over time?\nWhat are the potential benefits and limitations of relying on time series analysis to make predictions about stock market trends based on macroeconomic factors, and how can these limitations be addressed?\nHow do macroeconomic factors affect the stock market differently across different industries or sectors, and what strategies can be used to optimize investments based on these differences?\nHow do macroeconomic factors contribute to overall market volatility, and what steps can investors take to manage their exposure to market volatility based on these factors?\nWhat are some common mistakes or pitfalls that investors may encounter when trying to incorporate macroeconomic factors into their investment strategies, and how can these be avoided?\nHow can a comprehensive understanding of macroeconomic factors help investors build a diversified portfolio that is well-positioned to weather changes in the stock market over time?"
  },
  {
    "objectID": "ds.html",
    "href": "ds.html",
    "title": "Data Sources",
    "section": "",
    "text": "Analyzing historical returns on various investments is one of the most important tasks in financial markets. We need historical data for the assets to perform this analysis. There are numerous data providers, some of which are free while the majority are not. R Package designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. The Quantmod – “Quantitative Financial Modeling and Trading Framework for R”! package can load data, chart data, and generate relevant technical signals. This package is compatible with a variety of sources, including Yahoo Finance and FRED. By default, getSymbols() downloads data from Yahoo at the daily frequency and the object has 6 columns representing the open, high, low, close, volume and adjusted closing price for the stock."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Name: Chaitanya Shekar\nNet ID: cs2046\nHello everyone! My name is Chaitanya Shekar pursuing my masters degree in Data Science and Analytics at Georgetown University. I graduated with a bachelor’s degree with a major in statistics, mathematics, and economics. During the intensive programming component of my master’s degree, I’m learning skills and techniques to deal with unique challenges in complex algorithms and scientific problems. My DSAN goals involve strengthening my machine learning and deep learning understanding and programming skills.\nAs I take a quick look about, I see that the utilization of big data for business applications, establishing marketing trends, and forecasting consumer behavior has skyrocketed, and as a direct consequence of this, the demand for both business and data analysts has also considerably increased. I want to deepen my understanding of data science and statistics as well as have a better handle on the intricacies that are present in both areas of study. My high school placed a significant emphasis on mathematics and science, which kindled my interest in data and motivated me to pursue a degree in statistics while I was enrolled in my university studies. As a consequence of this, I aimed my qualifications in the right direction and worked on developing my analytical skills in order to acquire a deeper comprehension of the influence that data can wield.\nI consider myself to be an ambitious person who has the long-term goal of working in the field of data science at some point in the not-too-distant future. I would like to continue developing my talents in this area. In addition to completing the requirements for a conventional graduate degree, I also earned certificates in data science and analysis, machine learning, neural networks, deep learning, programming in Python and R, and all these subjects. Because I have such a great interest in data sciences, I decided to carry this out. To estimate who would emerge triumphant from a game of roulette based on statistical probability and game theory as the underlying concepts, I constructed a model-based project for the inter-collegiate Statistics Festival. During the time that I was an undergraduate student, I took part in a project in the field of data science that concentrated primarily on statistical principles. The study focused on cardiovascular health and the factors that are related with increased risk of cardiovascular disease.\nFollowing the completion of my undergraduate degree, I obtained an internship with Feedback Business Consulting Services Pvt, Bangalore, in the role of research analyst. As part of my responsibilities, I was responsible for managing the data gathering for the Patent Operations B2B benchmark research as well as performing analysis.\nAfter giving my current skill set and the demand for innovative analystâ€™s considerable consideration, I’ve come to the conclusion that I want to initiate the process of converting myself into a global citizen who is capable of thriving in high-pressure work environments. This course will provide me with the right steppingstone to enable me to enter the field of sophisticated technology and play a leadership role in the investigation and development of breakthroughs of this kind. This class will also prepare me to enter the field of sophisticated technology. Because of the meticulous study, analysis, and presentation of needs that the program provides, I am in a position where I can provide ideas and provide professional advice. I am extremely confident that it will assist me in applying the skills by undertaking projects and paving the path for ground-breaking research and development; furthermore, the emphasis on real-world skills, such as problem-solving, legal, ethical, and professional framework throughout the course that would give a holistic view. I am extremely confident that it will assist me in applying the skills by undertaking projects and paving the path for ground-breaking research and development. My head is spinning at the amazing amount of data and breadth that is necessary because I am detail-oriented, enjoys working with a variety of data, and appreciates performing analysis. The development of the capabilities and skills essential to prosper in the industry while also being able to adjust to the continuous changes that are taking place in that sector is the purpose of the program. In addition to my academic interests now, at a time when the global information technology industry is on the edge of a major technological transition, the data sciences are crucial to our attempts to explore the next frontier.\nI am quite proud of my academic achievements, but as a person, I am also aware of the advantages I have enjoyed in life, and I want to help close the gaps by making positive contributions toward creating a more equitable world. It motivated me to get involved in actions for social causes, such as becoming an active member of the core committee of Amnesty International. was an event that left me feeling really fulfilled and got me thinking about how I could put my skills to use in the service of social improvement. When I think back on my path, I realize that it was at that point that I first became aware of the applications of data sciences in a variety of fields, including but not limited to business, artificial intelligence, healthcare, sustainability, and climate change. During that time, I participated in several different events and campaigns in which I pushed for human rights, LGBTQ rights, and the rights of other underrepresented groups in our society. As part of my efforts to further the cause, I went to Home of Hope, a shelter for the disadvantaged, and while there, I became aware of socioeconomic and structural disparities. Not only that, but I also took part in efforts to raise general awareness about cancer and the silent march for AIDS. It was something I desired to investigate, and I dove headfirst into the data.\nIn addition to pursuing a career in academia, one of my passions is traveling and learning about new countries and ways of life. In addition, a significant reason for me to get involved in the fight against climate change and global warming is to promote sustainable living practices in all aspects of human endeavor."
  },
  {
    "objectID": "data_source.html",
    "href": "data_source.html",
    "title": "DATA SOURCES",
    "section": "",
    "text": "Analyzing historical returns on various investments is one of the most important tasks in financial markets. We need historical data for the assets to perform this analysis. There are numerous data providers, some of which are free while the majority are not. R Package designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. The Quantmod – “Quantitative Financial Modeling and Trading Framework for R”! package can load data, chart data, and generate relevant technical signals. This package is compatible with a variety of sources, including Yahoo Finance and FRED. By default, getSymbols() downloads data from Yahoo at the daily frequency and the object has 6 columns representing the open, high, low, close, volume and adjusted closing price for the stock.\nThe Yahoo Finance gives users immediate access to hours of live market coverage each day, complete with in-depth analysis and data. Investors, financial experts, and corporate executives who are serious about their money belong here.\n\nEXTRACTING DATA FROM R:\n\n\n\n\n\nGLOBAL STOCK INDICIES HISTORICAL DATA\nGlobal indices are a benchmark to evaluate the strength or weakness in the overall market. Normally, a sample of highly liquid and valuable stocks from the universe of listed stocks is selected and made into an index. The weighted movement of these set of stocks or portfolio of stocks constitutes the movement of global indices. So, if global indices are moving up that means the markets are strong and if global indices are moving lower that means global markets are weak. You can understand global indices as a hypothetical portfolio of investment holdings that represents a segment of the financial market or the global indices market. The calculation of the index value is derived from the prices of the underlying stocks or assets in the index.\nThe data is importing continent-wise, to check the top stock market index around the world.\n\nNorth America and South America\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n#America's top stock market index\ntickers = c(\"^GSPC\",\"^DJI\",\"^IXIC\", \"^RUT\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\namerica_stock_index_data = cbind(GSPC,DJI,IXIC,RUT)\namerica_stock_index_data = as.data.frame(america_stock_index_data)\n#export it to csv file\nwrite_csv(america_stock_index_data, \"DATA/RAW DATA/america_stock_index_data.csv\")\n\nstock <- data.frame(GSPC$GSPC.Adjusted,\n                    DJI$DJI.Adjusted,\n                    IXIC$IXIC.Adjusted,\n                    RUT$RUT.Adjusted)\n\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GSPC\",\"DJI\",\"IXIC\",\"RUT\",\"Dates\",\"date\")\n\n\n#remove columns\nstock <- stock[,-c(5)]\nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GSPC, colour=\"S&P 500 Index\"))+\n  geom_line(aes(y=DJI, colour=\"Dow Jones Industrial Average\"))+\n  geom_line(aes(y=IXIC, colour=\"NASDAQ Composite\"))+\n  geom_line(aes(y=RUT, colour=\"Russell 2000\"))+\n  scale_color_brewer(palette=\"RdPu\")+\n  theme_bw()+\n   labs(\n    title = \"America's Top Stock Market Index History\",\n    subtitle = \"From January 2000 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"America's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2000 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nEurope and Africa\n\n\n\nCode\n#Europe and Africa Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^GDAXI\",\"^FTSE\",\"^FCHI\", \"^IBEX\",\"^STOXX50E\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2018-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\neurope_africa_stock_index_data = cbind(GDAXI,FTSE,FCHI,IBEX,STOXX50E)\neurope_africa_stock_index_data = as.data.frame(europe_africa_stock_index_data)\n#export it to csv file\nwrite_csv(europe_africa_stock_index_data, \"DATA/RAW DATA/europe_africa_stock_index_data.csv\")\n\n\nstock <- data.frame(GDAXI$GDAXI.Adjusted,\n                    FTSE$FTSE.Adjusted,\n                    FCHI$FCHI.Adjusted,\n                    IBEX$IBEX.Adjusted,\n                    STOXX50E$STOXX50E.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GDAXI\",\"FTSE\",\"FCHI\",\"IBEX\",\"STOXX50E\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(6)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GDAXI, colour=\"DAX PERFORMANCE-INDEX\"))+\n  geom_line(aes(y=FTSE, colour=\"FTSE 100 Index\"))+\n  geom_line(aes(y=FCHI, colour=\"CAC 40\"))+\n  geom_line(aes(y=IBEX, colour=\"IBEX 35\"))+\n  geom_line(aes(y=STOXX50E, colour=\"EURO STOXX 50\"))+\n  scale_color_brewer(palette=\"ВuPu\")+\n  theme_bw()+\n   labs(\n    title = \"Europes and Africa's Top Stock Market Index History\",\n    subtitle = \"From January 2018 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Europes and Africa's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2018 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nAsia and Australia\n\n\n\nCode\n#Asia and Australia Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^N225\",\"^HSI\", \"^BSESN\",\"^NSEI\",\"^KS11\", \"^AORD\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\nasia_australia_stock_index_data = cbind(N225,HSI,BSESN,NSEI,AORD)\nasia_australia_stock_index_data = as.data.frame(asia_australia_stock_index_data)\n#export it to csv file\nwrite_csv(asia_australia_stock_index_data, \"DATA/RAW DATA/asia_australia_stock_index_data.csv\")\n\nstock <- data.frame(N225$N225.Adjusted,\n                    HSI$HSI.Adjusted,\n                    BSESN$BSESN.Adjusted,\n                    NSEI$NSEI.Adjusted,\n                    KS11$KS11.Adjusted,\n                    AORD$AORD.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"N225\",\"HSI\",\"BSESN\",\"NSEI\",\"KS11\",\"AORD\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(7)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=N225, colour=\"Nikkei 225\"))+\n  geom_line(aes(y=HSI, colour=\"Hang Seng Index\"))+\n  geom_line(aes(y=BSESN, colour=\"BSE SENSEX\"))+\n  geom_line(aes(y=NSEI, colour=\"NIFTY 50\"))+\n  geom_line(aes(y=KS11, colour=\"KOSPI Composite Index\"))+\n  geom_line(aes(y=AORD, colour=\"ALL ORDINARIES\"))+\n  scale_color_brewer(palette=\"PuBu\")+\n  theme_bw()+\n   labs(\n    title = \"Asia's Top Stock Market Index History\",\n    subtitle = \"From January 2012 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Asia and Australia's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2012 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\n\nSECTOR MARKET DATA\nA stock market sector is a group of stocks that have a lot in common with each other, usually because they are in similar industries. There are 11 different stock market sectors, according to the most commonly used classification system: the Global Industry Classification Standard (GICS)."
  },
  {
    "objectID": "dv_sector.html#bitcoin-plot-using-plotly",
    "href": "dv_sector.html#bitcoin-plot-using-plotly",
    "title": "Data Visualization",
    "section": "Bitcoin plot using plotly",
    "text": "Bitcoin plot using plotly\nOR you can obtain a single stock price\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n\n\n[1] \"2021-09-15\"\n\n\n[1] \"2023-02-16\"\n\n\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n           rownames.bitc.\n2021-09-15     2021-09-15\n2021-09-16     2021-09-16\n2021-09-17     2021-09-17\n2021-09-20     2021-09-20\n2021-09-21     2021-09-21\n2021-09-22     2021-09-22\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n                 date\n2021-09-15 2021-09-15\n2021-09-16 2021-09-16\n2021-09-17 2021-09-17\n2021-09-20 2021-09-20\n2021-09-21 2021-09-21\n2021-09-22 2021-09-22\n\n\n'data.frame':   359 obs. of  7 variables:\n $ BTC.Open    : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.High    : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Low     : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.Close   : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Volume  : num  102 877 502 110 464 ...\n $ BTC.Adjusted: num  99.7 99.6 99.5 99.6 99.6 ...\n $ date        : Date, format: \"2021-09-15\" \"2021-09-16\" ...\n\n\n\n\n\n\n\n\n\nThe stock price of Bitcoin diminishes with time.\n\n\n\n\n\n\nThe plot above shows that the stock price goes up and down. First, the stock price of Bitcoin drops, but it quickly goes back up to the same level.\n\nData Visualization with Climate Data\n\n\n\n\n\n\nThe line graph suggests that there could be a yearly seasonal trend. However, due to small dataset, we are unable to confirm this. From the graph, we may conclude that rainfall increases with rising temperatures and decreases with falling ones."
  },
  {
    "objectID": "dv.html#data-visualization-for-us-stock-indices",
    "href": "dv.html#data-visualization-for-us-stock-indices",
    "title": "Data Visualization",
    "section": "Data Visualization for US Stock Indices",
    "text": "Data Visualization for US Stock Indices\nStock market indexes all over the world are good measures of both the world economy and the economies of individual countries. In the United States, the S&P 500, the Dow Jones Industrial Average, and the Nasdaq Composite get the most attention from investors and the media. There are more than just these three indexes that make up the U.S. stock market. About 5,000 more are there.\nWith so many indexes, the U.S. market has many ways to classify things and methods that can be used for many different things. Most of the time, the news tells us several times a day how the top three indexes are going, using important news stories to show how they are going up or down. Investment managers use indexes to measure how well an investment is doing.\nIndexes are used by all types of investors as proxies for performance and guides for how to put their money to work. Indexes are also the basis for passive index investing, which is usually done through exchange-traded funds that track indexes. Overall, knowing how market indexes are made and how they are used can make many different types of investing easier to understand.\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n#America's top stock market index\ntickers = c(\"^GSPC\",\"^DJI\",\"^IXIC\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\ndv_america_stock_index_data = cbind(GSPC,DJI,IXIC)\ndv_america_stock_index_data = as.data.frame(dv_america_stock_index_data)\n#export it to csv file\nwrite_csv(dv_america_stock_index_data, \"DATA/RAW DATA/dv_america_stock_index_data.csv\")\n\nstock <- data.frame(GSPC$GSPC.Adjusted,\n                    DJI$DJI.Adjusted,\n                    IXIC$IXIC.Adjusted)\n\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GSPC\",\"DJI\",\"IXIC\",\"Dates\",\"date\")\n\n\n#remove columns\nstock <- stock[,-c(4)]\n\ng1<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GSPC, colour=\"GSPC\"))+\n  geom_line(aes(y=DJI, colour=\"DJI\"))+\n  geom_line(aes(y=IXIC, colour=\"IXIC\"))+\n  scale_color_brewer(palette=\"Greens\")+\n  theme_bw()+\n   labs(\n    title = \"America's Top 3 Stock Market Index History\",\n    subtitle = \"From Jan 2000-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\nplot = ggplotly(g1)%>%\n  layout(title = list(text = paste0(\"America's Top 3 Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2000-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\n\nEach of the indices’ stock prices tend to have a upward trend. This is because the companies in each of the indices are growing. Compared to S&P 500 and NASDAQ, the Dow Jones index has the largest share. This could be because of the companies in each index and how many companies are in each. The effect of covud on all businesses causes stock prices to drop at the beginning of 2020.\n\nDOW Jones Index\nThe Dow Jones Industrial Average (DJIA) is one of the oldest, best-known, and most-used indexes in the world. It has the shares of 30 of the biggest and most powerful companies in the US.The DJIA is an index based on prices. At first, it was made by adding up the price per share of each company’s stock in the index and dividing by the number of companies. The index is no longer this easy to figure out, though. Over time, stock splits, spin-offs, and other things have changed the divisor, which is a number that Dow Jones uses to figure out the level of the DJIA. This has made the divisor a very small number.\nAbout a quarter of the value of the whole U.S. stock market is represented by the DJIA, but a percent change in the Dow is not a sure sign that the whole market has dropped by the same percent. When the Dow goes up or down, it shows how investors feel about the earnings and risks of the big companies in the index. Because the way people feel about large-cap stocks is often different from how they feel about small-cap stocks, international stocks, or technology stocks, the Dow shouldn’t be used to show how people feel about other types of stocks in the market.\nIn general, the Dow is known for having a list of the best blue-chip companies on the U.S. market that pay regular dividends. So, it doesn’t have to be a reflection of the whole market, but it can be a reflection of the market for blue-chip, dividend-value stocks.\n\nList of Top 10 DOW Jones Companies by Weight\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\n\nUnitedHealth Group Incorporated\nUNH\n9.531581\n\n\nGoldman Sachs Group, Inc.\nGS\n7.240363\n\n\nHome Depot Inc.\nHD\n6.282804\n\n\nMcDonald’s Corporation\nMCD\n5.199097\n\n\nMicrosoft Corporation\nMSFT\n5.127123\n\n\nCaterpillar Inc.\nCAT\n4.821433\n\n\nAmgen Inc.\nAMGN\n4.580870\n\n\nVisa Inc. Class A\nV\n4.416778\n\n\nBoeing Company\nBA\n4.150398\n\n\nHoneywell International Inc.\nHON\n3.899078\n\n\n\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"UNH\",\"GS\",\"HD\", \"MCD\",\"MSFT\",\"CAT\",\"AMGN\",\"V\",\"BA\",\"HON\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(UNH$UNH.Adjusted,\n                    GS$GS.Adjusted,\n                    HD$HD.Adjusted,\n                    MCD$MCD.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    CAT$CAT.Adjusted,\n                    AMGN$AMGN.Adjusted,\n                    V$V.Adjusted,\n                    HON$HON.Adjusted,\n                    BA$BA.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng1<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=GS, colour=\"GS\"))+\n  geom_line(aes(y=HD, colour=\"HD\"))+\n  geom_line(aes(y=MCD, colour=\"MCD\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=CAT, colour=\"CAT\"))+\n  geom_line(aes(y=AMGN, colour=\"AMGN\"))+\n  geom_line(aes(y=V, colour=\"V\"))+\n  geom_line(aes(y=HON, colour=\"HON\"))+\n  geom_line(aes(y=BA, colour=\"BA\"))+\n  scale_color_brewer(palette=\"OrRd\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 Dow Jones Companies\",\n    subtitle = \"From Jan 2012-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g1)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 Dow Jones Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2012-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\n\nHere, the top 10 companies in the Dow Jones Index are shown as a time series. The companies are sorted by how much they make up the index. As it is clear that all the companies tend to go up, we can see that UNH has the biggest share of stock prices compared to the other companies. There has been a drop in the price of Home Depot Inc.’s stock, which was one of the best-performing stocks from 2018 to 2020. However, the drop may have been caused more by macro conditions and negative sentiment than by problems with the company itself or investors’ worries about a slowdown in the home improvement market. Because of the effect of covid, the price of UNH stock is going up. People started investing in health insurance, but the price has gone up and down because of this. Goldman Sachs Group, Inc. has a good stock price, but the price has gone up and down because of the pandemic in the fourth quarter. This was caused by weakness in investment banking and asset management, as well as a large loss in the unit that includes its consumer banking business.\n\n\n\nNASDAQ Composite Index\nMost investors know that the Nasdaq is where tech stocks trade. The Nasdaq Composite Index is a list of all the stocks that are traded on the Nasdaq stock exchange. It is based on how much each stock is worth on the market. Some of the companies in this index are not from the U.S. People know that this index has a lot of tech companies in it. It has things from the tech market like software, biotech, semiconductors, and more.\nThere are a lot of technology stocks in this index, but there are also stocks from other industries. Investors can also buy securities from a wide range of industries, such as financials, industrials, insurance, transportation, and others.\nThere are both big and small companies in the Nasdaq Composite. However, unlike the Dow and the S&P 500, it also has a lot of small, risky companies. So, its movement is usually a good sign of how well the technology industry is doing and how investors feel about riskier stocks.\n\nList of Top NASDAQ Companies by Weight\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\n\nApple Inc\nAAPL\n12.230\n\n\nMicrosoft Corp\nMSFT\n12.101\n\n\nAmazon.com Inc\nAMZN\n6.226\n\n\nNVIDIA Corp\nNVDA\n4.366\n\n\nTesla Inc\nTSLA\n3.967\n\n\nAlphabet Inc\nGOOG\n3.625\n\n\nAlphabet Inc\nGOOGL\n3.616\n\n\nMeta Platforms Inc\nMETA\n3.128\n\n\nBroadcom Inc\nAVGO\n1.962\n\n\nPepsiCo Inc\nPEP\n1.951\n\n\n\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"AAPL\",\"MSFT\",\"AMZN\", \"NVDA\",\"TSLA\",\"GOOG\",\"PEP\",\"GOOGL\",\"META\",\"AVGO\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2015-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(AAPL$AAPL.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    AMZN$AMZN.Adjusted,\n                    NVDA$NVDA.Adjusted,\n                    TSLA$TSLA.Adjusted,\n                    GOOG$GOOG.Adjusted,\n                    GOOGL$GOOGL.Adjusted,\n                    META$META.Adjusted,\n                    PEP$PEP.Adjusted,\n                    AVGO$AVGO.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng2<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=NVDA, colour=\"NVDA\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n  geom_line(aes(y=GOOG, colour=\"GOOG\"))+\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=META, colour=\"META\"))+\n  geom_line(aes(y=PEP, colour=\"PEP\"))+\n  geom_line(aes(y=AVGO, colour=\"AVGO\"))+\n  scale_color_brewer(palette=\"PuRd\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 NASDAQ Companies\",\n    subtitle = \"From Jan 2015-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g2)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 NASDAQ Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2015-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\n\nThe weights of the top 10 companies in the NASDAQ Index are used to filter the time series shown here. Broadcom Inc.’s stock price is high compared to others, but it goes up and down a lot. The price drop is mostly due to the fact that the company’s earnings growth will slow in fiscal 2019, while the price rise is due to its designs for data centers and networking. When you look at other companies, you can see that they all follow the same pattern. Most of them see a drop in their stock prices at the beginning of 2020, which is because of the covid. Google’s stock price has gone down recently, which is because its AI chatbot, Bard, gave a wrong answer.\n\n\n\nS&P 500 Index\nThe Standard & Poor’s 500 Index, or S&P 500, is a list of the 500 best companies in the United States. Stocks are chosen for the index based on their market capitalization, but the constituent committee also looks at their liquidity, public float, sector classification, financial stability, and trading history.\nThe S&P 500 Index contains about 80% of the total value of the U.S. stock market. The S&P 500 Index is a good way to get a general idea of how the whole U.S. market is doing. Most indexes are based on what something is worth on the market. The S&P 500 Index is the market-weighted index (also referred to as capitalization-weighted).\nSo, the weight of each stock in the index is the same as its total market capitalization. In other words, the value of the index falls by 10% if the market value of all 500 companies in the S&P 500 falls by 10%. ##### List of Top 10 S&P 500 Companies by Weight\n\n\n\n\n\nCompany\nSymbol\nWeight\n\n\n\n\nApple Inc.\nAAPL\n6.711304\n\n\nMicrosoft Corporation\nMSFT\n5.705910\n\n\nAmazon.com Inc.\nAMZN\n2.543539\n\n\nAlphabet Inc. Class A\nGOOGL\n1.665711\n\n\nBerkshire Hathaway Inc. Class B\nBRK.B\n1.621257\n\n\nNVIDIA Corporation\nNVDA\n1.601427\n\n\nTesla Inc\nTSLA\n1.583414\n\n\nAlphabet Inc. Class C\nGOOG\n1.480755\n\n\nExxon Mobil Corporation\nXOM\n1.391616\n\n\nUnitedHealth Group Incorporated\nUNH\n1.329559\n\n\n\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"AAPL\",\"MSFT\",\"AMZN\", \"NVDA\",\"TSLA\",\"GOOGL\",\"GOOG\",\"BRK\",\"UNH\",\"XOM\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2015-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(AAPL$AAPL.Adjusted,\n                    MSFT$MSFT.Adjusted,\n                    AMZN$AMZN.Adjusted,\n                    NVDA$NVDA.Adjusted,\n                    TSLA$TSLA.Adjusted,\n                    GOOG$GOOG.Adjusted,\n                    GOOGL$GOOGL.Adjusted,\n                    BRK$BRK.Adjusted,\n                    UNH$UNH.Adjusted,\n                    XOM$XOM.Adjusted)\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\n\ng3<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=AAPL, colour=\"AAPL\"))+\n  geom_line(aes(y=MSFT, colour=\"MSFT\"))+\n  geom_line(aes(y=AMZN, colour=\"AMZN\"))+\n  geom_line(aes(y=NVDA, colour=\"NVDA\"))+\n  geom_line(aes(y=TSLA, colour=\"TSLA\"))+\n  geom_line(aes(y=GOOG, colour=\"GOOG\"))+\n  geom_line(aes(y=GOOGL, colour=\"GOOGL\"))+\n  geom_line(aes(y=BRK, colour=\"BRK\"))+\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=XOM, colour=\"XOM\"))+\n  scale_color_brewer(palette=\"GnBu\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Prices for the Top 10 S&P 500 Companies\",\n    subtitle = \"From Jan 2015-Jan 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Companies\")) \n\nplot = ggplotly(g3)%>%\n  layout(title = list(text = paste0(\"Stock Prices for the Top 10 S&P 500 Companies\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From Jan 2015-Jan 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\n\nThe time series shown here is filtered by the weights of the top 10 companies in the S&P 500 Index. How much each company makes up the index is used to sort the companies. Since it’s clear that all the companies’ stock prices tend to go up, we can see that UNH has the most stock prices compared to the other companies. Most of their stock prices will go down at the start of 2020 because of the covid. Recently, Google’s stock price went down because its artificial intelligence chatbot, Bard, gave the wrong answer. The price of TESLA stock has been going down since early 2022. This is because investors worry that CEO Elon Musk is too busy with his plan to take over Twitter."
  },
  {
    "objectID": "dv.html#results",
    "href": "dv.html#results",
    "title": "Data Visualization",
    "section": "Results",
    "text": "Results\n\nPlots\nWe show a scatter plot in this section.\n\n\nCode\npar(mar = c(4, 4, .5, .1))\nplot(mpg ~ hp, data = mtcars, pch = 19)\n\n\n\n\n\n\n\nTables\nWe show the data in this tab.\n\n\nCode\nhead(mtcars)\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\n\nNASDAQ Composite Index\nMost investors are aware that technology stocks are traded on the Nasdaq. The Nasdaq Composite Index is an index of all the stocks that are traded on the Nasdaq stock exchange. It is based on the market capitalization of each stock. Some companies in this index are not based in the United States.\nThis index is known for having a lot of tech companies in it. It includes software, biotech, semiconductors, and more from the tech market. This index is known for having a lot of technology stocks in it, but it also has some stocks from other industries.\nInvestors can also find securities from a wide range of industries, such as financials, industrials, insurance, transportation, and others. The Nasdaq Composite has both big and small companies in it, but unlike the Dow and the S&P 500, it also has a lot of small, speculative companies. So, its movement is usually a good indicator of how well the technology industry is doing and how investors feel about riskier stocks.\nList of Top NASDAQ Companies by Weight\n\n\nS&P 500 Index\nThe S&P 500, or Standard & Poor’s 500 Index, is a list of 500 of the best companies in the U.S. Stocks are chosen for the index based on their market capitalization, but the constituent committee also looks at liquidity, public float, sector classification, financial stability, and trading history.\nAbout 80% of the total value of the U.S. stock market can be found in the S&P 500 Index. In general, the S&P 500 Index is a good way to see how the whole U.S. market is doing.\nMost indexes are based on market value or price. The market-weighted index is the S&P 500 Index (also referred to as capitalization-weighted). So, each stock in the index has the same amount of weight in the index as its total market capitalization. In other words, if the market value of all 500 companies in the S&P 500 falls by 10%, so does the value of the index.\nList of Top 10 S&P 500 Companies by Weight"
  },
  {
    "objectID": "dv.html#data-visualization-for-sector-stock-market",
    "href": "dv.html#data-visualization-for-sector-stock-market",
    "title": "Data Visualization",
    "section": "Data Visualization for Sector Stock Market",
    "text": "Data Visualization for Sector Stock Market\nOn the stock market, a sector is a group of stocks that are all in the same industry and are very similar to each other. The Global Industrial Classification Standard, which is the most common way to group things, says that there are 11 different stock market sectors (GICS).\n\n\n\n\n\nName\nSymbol\n\n\n\n\nConsumer Staples Sector Fund\nXLP\n\n\nUtilities Sector Fund\nXLU\n\n\nHealth Care Sector Fund\nXLV\n\n\nIndustrial Sector Fund\nXLI\n\n\nFinancial Sector Fund\nXLF\n\n\nConsumer Discretionary Sector Fund\nXLY\n\n\nCommunication Services Sector Fund\nXLC\n\n\nReal Estate Sector Fund\nXLRE\n\n\nMaterials Sector Fund\nXLB\n\n\nTechnology Sector Fund\nXLK\n\n\nEnergy Sector Fund\nXLE\n\n\n\n\n\n\n\nView the visualization\n\nThere are 11 parts to the sector market, and each has companies with good stock prices. When we look at the graph, we can see that the Consumer Discretionary sector has higher stock prices than other sectors. This is because there is a chance for high returns, especially when the economy is doing well and consumers are spending a lot. Because they are near the middle of the risk spectrum, stocks in the Financial Sector are worth the least. They can be prone to recessions and are sensitive to changes in interest rates, to name just two major risks. But like most other kinds of businesses, the risk of bank stocks can vary a lot from one company to the next. We can also guess that most sector prices have been going up since 2021, which could be because of post covid."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "EDA test will be executed for the top most company in each of the indices which are UnitedHealth Group Incorporated, Apple and Microsoft and the Macroeconomic Factors."
  },
  {
    "objectID": "eda.html#microsoft",
    "href": "eda.html#microsoft",
    "title": "Exploratory Data Analysis",
    "section": "Microsoft",
    "text": "Microsoft\n\nTime Series Plot\n\n\nCode\n# get data\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n\ndata = getSymbols(\"MSFT\",src='yahoo',from = '2015-01-01',to = \"2023-03-01\")\n\ndf <- data.frame(Date=index(MSFT),coredata(MSFT))\n\n# create Bollinger Bands\nbbands <- BBands(MSFT[,c(\"MSFT.High\",\"MSFT.Low\",\"MSFT.Close\")])\n\n# join and subset data\ndf <- subset(cbind(df, data.frame(bbands[,1:3])), Date >= \"2015-01-01\")\n\n#export data\nmicrosoft_raw_data <- df\nwrite.csv(microsoft_raw_data, \"DATA/CLEANED DATA/microsoft_raw_data.csv\", row.names=FALSE)\n\n#convert data to ts data\nmyts<-ts(df$MSFT.Adjusted,frequency=365,start=c(2015,1,1)) \n\n#export data\nmicrosoft_ts_data <- myts\nwrite.csv(microsoft_ts_data, \"DATA/CLEANED DATA/microsoft_ts_data.csv\", row.names=FALSE)\n\n# colors column for increasing and decreasing\nfor (i in 1:length(df[,1])) {\n  if (df$MSFT.Close[i] >= df$MSFT.Open[i]) {\n      df$direction[i] = 'Increasing'\n  } else {\n      df$direction[i] = 'Decreasing'\n  }\n}\n\ni <- list(line = list(color = '#EBD168'))\nd <- list(line = list(color = '#7F7F7F'))\n\n# plot candlestick chart\n\nfig <- df %>% plot_ly(x = ~Date, type=\"candlestick\",\n          open = ~MSFT.Open, close = ~MSFT.Close,\n          high = ~MSFT.High, low = ~MSFT.Low, name = \"MSFT\",\n          increasing = i, decreasing = d) \nfig <- fig %>% add_lines(x = ~Date, y = ~up , name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\",\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% add_lines(x = ~Date, y = ~dn, name = \"B Bands\",\n            line = list(color = '#ccc', width = 0.5),\n            legendgroup = \"Bollinger Bands\", inherit = F,\n            showlegend = FALSE, hoverinfo = \"none\") \nfig <- fig %>% add_lines(x = ~Date, y = ~mavg, name = \"Mv Avg\",\n            line = list(color = '#E377C2', width = 0.5),\n            hoverinfo = \"none\", inherit = F) \nfig <- fig %>% layout(yaxis = list(title = \"Price\"))\n\n# plot volume bar chart\nfig2 <- df \nfig2 <- fig2 %>% plot_ly(x=~Date, y=~MSFT.Volume, type='bar', name = \"MSFT Volume\",\n          color = ~direction, colors = c('#EBD168','#7F7F7F')) \nfig2 <- fig2 %>% layout(yaxis = list(title = \"Volume\"))\n\n# create rangeselector buttons\nrs <- list(visible = TRUE, x = 0.5, y = -0.055,\n           xanchor = 'center', yref = 'paper',\n           font = list(size = 9),\n           buttons = list(\n             list(count=1,\n                  label='RESET',\n                  step='all'),\n             list(count=1,\n                  label='1 YR',\n                  step='year',\n                  stepmode='backward'),\n             list(count=3,\n                  label='3 MO',\n                  step='month',\n                  stepmode='backward'),\n             list(count=1,\n                  label='1 MO',\n                  step='month',\n                  stepmode='backward')\n           ))\n\n# subplot with shared x axis\nfig <- subplot(fig, fig2, heights = c(0.7,0.2), nrows=2,\n             shareX = TRUE, titleY = TRUE)\nfig <- fig %>% layout(title = paste(\"Microsoft Stock Price: January 2015 - March 2023\"),\n         xaxis = list(rangeselector = rs),\n         legend = list(orientation = 'h', x = 0.5, y = 1,\n                       xanchor = 'center', yref = 'paper',\n                       font = list(size = 10),\n                       bgcolor = 'transparent'))\n\nfig\n\n\n\n\n\n\nThe above graph, clearly indicates that there is upward trend un the stock price of Microsoft from 2015 to 2023. Share prices are generally trending upwards. There are some ups and downs of various sizes. Past few years there has been fluctuation in the stock price is because economic challenges fueled Microsoft’s fall from grace. High inflation slowed revenue growth and supercharged operating expenses, a one-two punch that led to a disappointing financial performance over the past year. Since there is seasonal variations keep on increasing proportionally to the trend, multiplicative decomposition method is recommended.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Adjusted Closing Price\", main = \"Microsoft Stock price: Jan 2015 - March 2023\")\ndecompose = decompose(myts, \"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='trend') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component has an upward trend and greater variability in the model, but the adjusted trend component has a stable trend through time.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for Microsoft Stock Jan 2015 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- df %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(MSFT.Adjusted))\n\n#ts for month data\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2015-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for month\nts_lags(month)\n\n\n\n\n\n\n\n\n\nMicrosoft Stock Lag Plot As there is a positive correlation and an inclination angle of 45 degrees, there should be a significant link between the series and the relevant lag from January 2015 to March 2023. This is the lag plot hallmark of a process that has a high degree of positive autocorrelation. Such processes are highly non-random—there is a substantial relationship between one observation and the next. Seasonality can also be investigated by plotting observations for a wider number of time periods, i.e. the lags. The time series data is aggregated to monthly data using the mean function for a better comprehension of the series and crisper plots. Further inspection of the last graph reveals that more dots are on the diagonal line at 45 degrees. The second graph shows the monthly variation of the variable on the vertical axis. In chronological order, the lines connect the points. The connection is strongly positive, confirming the data’s strong seasonality, and there is a correlation.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing - 4 month\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,5), series=\"4 month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Microsoft Stock Jan 2015 - March 2023 (4 Month Moving Average\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 1 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Microsoft Stock Jan 2015 - March 2023 (1 Year Moving Average\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 3 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Microsoft Stock Jan 2015 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing - 5 Year\nma <- autoplot(month, series=\"Data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Microsoft Stock Jan 2015 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe four plots above show the Microsoft stock prices for the period between Jan 2015 and March 2023, smoothed using 4-month, 1-year, 3-year, and 5-year moving averages.\nLooking at the plots, we can see that the moving average values are increasing over time. The 4-month MA plot shows a lot of fluctuations, which is expected because it captures the short-term variations in the stock prices. On the other hand, the 1-year, 3-year, and 5-year MA plots smooth out the fluctuations and show the overall trend of the stock prices.\nWe can observe that the 5-year MA plot provides a smoother trend as it takes into account a longer time period compared to the other plots. The 3-year MA plot also provides a relatively smooth trend, but it captures shorter-term variations compared to the 5-year MA plot. The 1-year MA plot is even more sensitive to shorter-term variations in the stock prices. From the moving average obtained above we can see that there is upward tend in the stock price of Microsoft.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(month)+ggtitle(\"ACF Plot for Microsoft Monthly Stock Jan 2015 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(month)+ggtitle(\"PACF Plot for Microsoft Monthly Stock Jan 2015 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.5937, Lag order = 4, p-value = 0.3309\nalternative hypothesis: stationary\n\n\n\n\n\nThere is clear autocorrelation in lag in the plot of autocorrelation function, which is the acf graph for monthly data. The lag plots and autocorrelation plots shown above suggest seasonality in the series, indicating that it is not stationary. It was also validated using the Augmented Dickey-Fuller Test, which indicates that the series is not stationary as the p value is more than 0.05.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-62.972 -22.213  -6.185  20.109  97.336 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -1.053e+05  8.101e+02  -129.9   <2e-16 ***\ntime(myts)   5.224e+01  4.015e-01   130.1   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.53 on 2051 degrees of freedom\nMultiple R-squared:  0.8919,    Adjusted R-squared:  0.8919 \nF-statistic: 1.693e+04 on 1 and 2051 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Microsoft Stock Price\")\nplot2 <- ggAcf(resid(fit), 48, main=\"detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"first differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 5.224e+01. With a standard error of 4.015e-01, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(1.053e+05)-(5.224e+01)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda.html#conclusion",
    "href": "eda.html#conclusion",
    "title": "Exploratory Data Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nAccording to the EDA for the top three firms with significant stock prices throughout the world, there has been a fluctuation in the trend and a large decrease and rise in the stock price over the current period, and each of the companies has its own explanation for the fluctuation."
  },
  {
    "objectID": "ds.html#global-stock-indices-historical-data",
    "href": "ds.html#global-stock-indices-historical-data",
    "title": "Data Sources",
    "section": "Global Stock Indices Historical Data",
    "text": "Global Stock Indices Historical Data\nGlobal indices are a benchmark to evaluate the strength or weakness in the overall market. Normally, a sample of highly liquid and valuable stocks from the universe of listed stocks is selected and made into an index. The weighted movement of these set of stocks or portfolio of stocks constitutes the movement of global indices. So, if global indices are moving up that means the markets are strong and if global indices are moving lower that means global markets are weak. You can understand global indices as a hypothetical portfolio of investment holdings that represents a segment of the financial market or the global indices market. The calculation of the index value is derived from the prices of the underlying stocks or assets in the index.\nThe data is importing continent-wise, to check the top stock market index around the world.\n\nNorth America and South AmericaEurope and AfricaAsia and Australia\n\n\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n#America's top stock market index\ntickers = c(\"^GSPC\",\"^DJI\",\"^IXIC\", \"^RUT\")\n\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2000-01-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\namerica_stock_index_data = cbind(GSPC,DJI,IXIC,RUT)\namerica_stock_index_data = as.data.frame(america_stock_index_data)\n#export it to csv file\nwrite_csv(america_stock_index_data, \"DATA/RAW DATA/america_stock_index_data.csv\")\n\nstock <- data.frame(GSPC$GSPC.Adjusted,\n                    DJI$DJI.Adjusted,\n                    IXIC$IXIC.Adjusted,\n                    RUT$RUT.Adjusted)\n\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GSPC\",\"DJI\",\"IXIC\",\"RUT\",\"Dates\",\"date\")\n\n\n#remove columns\nstock <- stock[,-c(5)]\nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GSPC, colour=\"S&P 500 Index\"))+\n  geom_line(aes(y=DJI, colour=\"Dow Jones Industrial Average\"))+\n  geom_line(aes(y=IXIC, colour=\"NASDAQ Composite\"))+\n  geom_line(aes(y=RUT, colour=\"Russell 2000\"))+\n  scale_color_brewer(palette=\"RdPu\")+\n  theme_bw()+\n   labs(\n    title = \"America's Top Stock Market Index History\",\n    subtitle = \"From January 2000 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nplot = ggplotly(plot)%>%\n  layout(title = list(text = paste0(\"America's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2000 - January 2023',\n                                    '</sup>')))\nggplotly(plot)%>%layout(hovermode = \"x\")\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#Europe and Africa Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^GDAXI\",\"^FTSE\",\"^FCHI\", \"^IBEX\",\"^STOXX50E\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2018-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\neurope_africa_stock_index_data = cbind(GDAXI,FTSE,FCHI,IBEX,STOXX50E)\neurope_africa_stock_index_data = as.data.frame(europe_africa_stock_index_data)\n#export it to csv file\nwrite_csv(europe_africa_stock_index_data, \"DATA/RAW DATA/europe_africa_stock_index_data.csv\")\n\n\nstock <- data.frame(GDAXI$GDAXI.Adjusted,\n                    FTSE$FTSE.Adjusted,\n                    FCHI$FCHI.Adjusted,\n                    IBEX$IBEX.Adjusted,\n                    STOXX50E$STOXX50E.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"GDAXI\",\"FTSE\",\"FCHI\",\"IBEX\",\"STOXX50E\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(6)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=GDAXI, colour=\"DAX PERFORMANCE-INDEX\"))+\n  geom_line(aes(y=FTSE, colour=\"FTSE 100 Index\"))+\n  geom_line(aes(y=FCHI, colour=\"CAC 40\"))+\n  geom_line(aes(y=IBEX, colour=\"IBEX 35\"))+\n  geom_line(aes(y=STOXX50E, colour=\"EURO STOXX 50\"))+\n  scale_color_brewer(palette=\"ВuPu\")+\n  theme_bw()+\n   labs(\n    title = \"Europes and Africa's Top Stock Market Index History\",\n    subtitle = \"From January 2018 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Europes and Africa's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2018 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#Asia and Australia Top Indices\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\ntickers = c(\"^N225\",\"^HSI\", \"^BSESN\",\"^NSEI\",\"^KS11\", \"^AORD\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2012-01-01\",\n             to = \"2023-01-01\", periodicity=\"monthly\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\n#create dataframe\nasia_australia_stock_index_data = cbind(N225,HSI,BSESN,NSEI,AORD)\nasia_australia_stock_index_data = as.data.frame(asia_australia_stock_index_data)\n#export it to csv file\nwrite_csv(asia_australia_stock_index_data, \"DATA/RAW DATA/asia_australia_stock_index_data.csv\")\n\nstock <- data.frame(N225$N225.Adjusted,\n                    HSI$HSI.Adjusted,\n                    BSESN$BSESN.Adjusted,\n                    NSEI$NSEI.Adjusted,\n                    KS11$KS11.Adjusted,\n                    AORD$AORD.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\ncolnames(stock)=c(\"N225\",\"HSI\",\"BSESN\",\"NSEI\",\"KS11\",\"AORD\",\"Dates\",\"date\")\n#remove columns\nstock <- stock[,-c(7)] \nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=N225, colour=\"Nikkei 225\"))+\n  geom_line(aes(y=HSI, colour=\"Hang Seng Index\"))+\n  geom_line(aes(y=BSESN, colour=\"BSE SENSEX\"))+\n  geom_line(aes(y=NSEI, colour=\"NIFTY 50\"))+\n  geom_line(aes(y=KS11, colour=\"KOSPI Composite Index\"))+\n  geom_line(aes(y=AORD, colour=\"ALL ORDINARIES\"))+\n  scale_color_brewer(palette=\"PuBu\")+\n  theme_bw()+\n   labs(\n    title = \"Asia's Top Stock Market Index History\",\n    subtitle = \"From January 2012 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Indices\")) \n\n  \n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0(\"Asia and Australia's Top Stock Market Index History\",\n                                    '<br>',\n                                    '<sup>',\n                                    'From January 2012 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "ds.html#sector-market-data",
    "href": "ds.html#sector-market-data",
    "title": "Data Sources",
    "section": "Sector Market Data",
    "text": "Sector Market Data\nA stock market sector is a group of stocks that have a lot in common with each other, usually because they are in similar industries. There are 11 different stock market sectors, according to the most commonly used classification system: the Global Industry Classification Standard (GICS).\n\n\nCode\nlibrary(wesanderson)\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"XLC\",\"XLY\",\"XLP\", \"XLE\", \"XLF\", \"XLV\",\"XLI\",\"XLB\", \"XLRE\", \"XLK\",\"XLU\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2018-07-01\",\n             to = \"2023-01-01\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(XLC$XLC.Adjusted,\n                    XLY$XLY.Adjusted,\n                    XLP$XLP.Adjusted,\n                    XLE$XLE.Adjusted,\n                    XLF$XLF.Adjusted,\n                    XLV$XLV.Adjusted,\n                    XLI$XLI.Adjusted,\n                    XLB$XLB.Adjusted,\n                    XLRE$XLRE.Adjusted,\n                    XLK$XLK.Adjusted,\n                    XLU$XLU.Adjusted)\n\n#create dataframe\nsector_stock_data = cbind(XLC,XLY,XLP, XLE, XLF, XLV,XLI,XLB, XLRE, XLK,XLU)\nsector_stock_data = as.data.frame(sector_stock_data)\n#export it to csv file\nwrite_csv(sector_stock_data, \"DATA/RAW DATA/sector_stock_data.csv\")\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\n\nplot<- ggplot(stock, aes(x=date)) +\n  geom_line(aes(y=XLC, colour=\"Communication Services\"))+\n  geom_line(aes(y=XLY, colour=\"Consumer Discretionary\"))+\n  geom_line(aes(y=XLP, colour=\"Consumer Staples\"))+\n  geom_line(aes(y=XLE, colour=\"Energy\"))+\n  geom_line(aes(y=XLF, colour=\"Financials\"))+\n  geom_line(aes(y=XLV, colour=\"Health Care\"))+\n  geom_line(aes(y=XLI, colour=\"Industrials\"))+\n  geom_line(aes(y=XLB, colour=\"Materials\"))+\n  geom_line(aes(y=XLRE, colour=\"Real Estate\"))+\n  geom_line(aes(y=XLK, colour=\"Technology\"))+\n  geom_line(aes(y=XLU, colour=\"Utilities\"))+\n  scale_color_brewer(palette=\"BuPu\")+\n  theme_bw()+\n   labs(\n    title = \"Stock Market Sector History\",\n    subtitle = \"From July 2018 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Sectors\")) \n\n  \n\n\n\nggplotly(plot)%>%\n  layout(title = list(text = paste0('Stock Market Sector History',\n                                    '<br>',\n                                    '<sup>',\n                                    'From July 2018 - January 2023',\n                                    '</sup>')))\n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "ds.html#macroeconomic-factors-data",
    "href": "ds.html#macroeconomic-factors-data",
    "title": "Data Sources",
    "section": "Macroeconomic Factors Data",
    "text": "Macroeconomic Factors Data\nMacroeconomic factors such as inflation rates, gross domestic product (GDP), unemployment rates, and interest rates have a significant impact on financial markets. Historical data on these factors is crucial for analyzing and predicting market trends.\n\nThe Federal Reserve Economic Data\nThe Federal Reserve Economic Data (FRED) website, maintained by the Federal Reserve Bank of St. Louis, is a reliable source for GDP growth rate data. It offers a range of data frequencies and time periods, with a user-friendly interface featuring interactive charts and graphs to visualize trends and patterns. The website also provides tools and resources to manipulate and analyze the data, making it a valuable resource for researchers, analysts, and policymakers alike. You can access this data at https://fred.stlouisfed.org/.\n\n\nU.S. Bureau of Labor Statistics\nThe U.S. Bureau of Labor Statistics (BLS) is a government agency that collects and disseminates data related to labor economics and statistics. One of the key data sets provided by the BLS is information on employment, unemployment, and wages. This data is widely used by economists, policymakers, and businesses to analyze labor market trends and make informed decisions. Additionally, the BLS provides data on other economic indicators such as inflation and productivity. The agency’s commitment to providing accurate, reliable, and timely data has made it a trusted source for labor market information in the United States. You can access this data at https://www.bls.gov/.\n\nAbout the Macroeconomic Factor Data\nThe economic data related to inflation, unemployment, GDP growth rate and interest rate are crucial indicators of the health of an economy. Inflation rate measures the general increase in prices of goods and services over time, which is an essential measure of price stability. The unemployment rate measures the percentage of people who are seeking employment but are unable to find it, indicating the level of economic activity and labor market health. GDP growth rate measures the change in the value of goods and services produced in an economy over time and is an indicator of economic performance. Interest rates measure the cost of borrowing money, and changes in interest rates can have significant impacts on consumer spending, investment, and borrowing decisions.\n\nGDP Growth RateInterest RateInflation RateUnemployment Rate\n\n\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$DATE <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#plot gdp growth rate\nfig <- plot_ly(gdp, x = ~DATE, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(158,202,225)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Year\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(222, 92, 92)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#45818E\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_minimal()\nggplotly(fig)\n\n\n\n\n\n\n\nDownload CSV\n\n\n\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n#plot unemployment rate \n#plot interest rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(135, 153, 164)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\n\nDownload CSV"
  },
  {
    "objectID": "dv.html#data-visualization-for-macroeconomic-factors",
    "href": "dv.html#data-visualization-for-macroeconomic-factors",
    "title": "Data Visualization",
    "section": "Data Visualization for macroeconomic factors",
    "text": "Data Visualization for macroeconomic factors\n\nGross Domestic Product Growth Rate\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$DATE <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~DATE, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(179, 210, 165)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\nThe period between 2010 and 2022 has been characterized by moderate fluctuations in economic growth. After a period of slow growth following the 2008 financial crisis, the US economy experienced a notable uptick in GDP growth from 2012 to 2015, with rates reaching a peak of 2.9% in 2015. However, growth rates began to decline again from 2016 to 2019, falling to a low of 2.2% in 2019. The COVID-19 pandemic caused a sharp contraction in the economy in 2020, with GDP growth falling by 3.5%, the largest annual decline since the 1940s. However, there was a partial recovery in 2021, with growth rates projected to reach 6.3% by the end of the year, reflecting a combination of fiscal stimulus measures and the easing of pandemic-related restrictions. Overall, the trend in GDP growth rate in the United States from 2010 to 2022 has been characterized by moderate fluctuations, with notable shifts in response to both domestic and global economic conditions.\n\n\nInterest Rate\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(59, 14, 37)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\nThe graph of the real interest rate in the US from 2010 to March 2023 shows a general trend of volatility and fluctuation. The real interest rate is a measure of the cost of borrowing for the US government and is adjusted for inflation.\nFrom 2010 to mid-2012, the real interest rate remained relatively low and stable, with only minor fluctuations.This increase in the real interest rate was likely due to concerns about inflation and the impact of the US government’s monetary policy measures. From mid-2013 to mid-2016, the real interest rate remained relatively low, with only minor fluctuations. However, from mid-2016 to mid-2018, there was another sharp increase in the real interest rate. This increase was driven by a combination of factors, including the improving US economy, rising inflation expectations, and the Federal Reserve’s decision to raise interest rates.\nFrom mid-2018 to mid-2019, the real interest rate declined sharply, and then remained relatively stable at lower levels until early 2021. This decline was largely due to concerns about a slowing global economy, trade tensions, and the impact of the COVID-19 pandemic. Since early 2021, the real interest rate has been increasing again, and it remains at a relatively high level as of March 2023. This increase may be due to concerns about inflation, the impact of government stimulus measures, and the possibility of an economic recovery.\nOverall, the trend of the real interest rate in the US from 2010 to March 2023 has been characterized by periods of volatility and fluctuation, driven by a range of economic and policy factors.\n\n\nInflation Rate\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#8B8695\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe U.S inflation rate from January 2010 to February 2023 has been a topic of concern for many individuals and businesses. From 2010 to 2012, the inflation rate remained relatively low. However, from 2013 to 2023, the inflation rate fluctuated significantly. The COVID-19 pandemic had a significant impact on the inflation rate, causing it to rise rapidly in 2021 due to supply chain disruptions and other factors. The Federal Reserve has implemented various policies to try and manage the inflation rate, including adjusting interest rates and reducing bond purchases. The U.S inflation rate remains a closely watched indicator of economic health, and its fluctuations can have significant impacts on individuals, businesses, and the broader economy.\n\n\nUnemployment Rate\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n#plot unemployment rate \n#plot interest rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(235, 231, 115)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\nThe U.S unemployment rate has experienced significant fluctuations between January 2010 and February 2023. Following the Great Recession of 2008, the unemployment rate peaked in October 2009, but gradually decreased to by January 2010. Throughout the years, the rate has continued to fluctuate, before increasing in April 2020 due to the COVID-19 pandemic. However, as the pandemic-related restrictions were lifted and the economy started to recover, the unemployment rate began to decline. Despite the progress made in recent years, the unemployment rate remains a significant economic indicator and a source of concern for policymakers, as persistent unemployment can have long-term effects on the economy and the well-being of individuals and families."
  },
  {
    "objectID": "eda.html#gross-domestic-product-growth-rate",
    "href": "eda.html#gross-domestic-product-growth-rate",
    "title": "Exploratory Data Analysis",
    "section": "Gross Domestic Product Growth Rate",
    "text": "Gross Domestic Product Growth Rate\n\nTime Series Plot\n\n\nCode\n#import the data\ngdp <- read.csv(\"DATA/RAW DATA/gdp-growth.csv\")\n\n#change date format\ngdp$Date <- as.Date(gdp$DATE , \"%m/%d/%Y\")\n\n#drop DATE column\ngdp <- subset(gdp, select = -c(1))\n\n#export the cleaned data\ngdp_clean <- gdp\nwrite.csv(gdp_clean, \"DATA/CLEANED DATA/gdp_clean_data.csv\", row.names=FALSE)\n\n#convert to ts data\nmyts<-ts(gdp$value,frequency=4,start=c(2010/1/1))\ngdp_ts<-myts\n#export the data\nwrite.csv(gdp_ts, \"DATA/CLEANED DATA/gdp_ts_data.csv\", row.names=FALSE)\n\n#plot gdp growth rate \nfig <- plot_ly(gdp, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(220,20,60)'))\nfig <- fig %>% layout(title = \"U.S GPD Growth Rate: 2010 - 2022\",xaxis = list(title = \"Time\"),yaxis = list(title =\"GDP Growth Rate\"))\nfig\n\n\n\n\n\n\nThe trend in GDP growth rate in the United States from 2010 to 2022 has been characterized by moderate fluctuations, reflecting a range of economic conditions and policy responses. Between 2010 and 2022, the United States experienced a range of GDP growth rates, reflecting various economic conditions and policy responses. In the early years of this period, the economy was still recovering from the 2008 financial crisis, which had led to a prolonged period of slow growth. In 2012, GDP growth began to pick up, reaching 2.8% that year, followed by 1.8% in 2013 and 2.5% in 2014. The peak in this period came in 2015, when the GDP growth rate reached 2.9%. However, the momentum of growth slowed in the years that followed. In 2016, GDP growth declined to 1.6%, followed by 2.2% in 2017, and 2.9% in 2018. By 2019, growth had slowed again to 2.2%. This period of slower growth was attributed to a range of factors, including the tightening of monetary policy by the Federal Reserve, global economic headwinds, and ongoing concerns about political instability and trade tensions. The COVID-19 pandemic in 2020 led to a sharp contraction in economic activity, with GDP growth declining by 3.5%, the largest annual decline since the 1940s. The pandemic resulted in widespread shutdowns of businesses, schools, and public spaces, as well as disruptions to global supply chains and trade. However, the US government and the Federal Reserve responded with a range of fiscal and monetary policies, including direct payments to households, increased unemployment benefits, and massive injections of liquidity into financial markets. These measures helped to mitigate the impact of the pandemic on the economy. In 2021, the US economy began to recover, with GDP growth projected to reach 6.3% by the end of the year. This rebound was due to a combination of factors, including the easing of pandemic-related restrictions, increased vaccination rates, and the continuation of government stimulus measures.\nThe GDP growth rate in the United States from 2010 to 2021, there appears to be some seasonality in the data. he seasonality appears to be relatively consistent over time, with spikes in GDP growth rate occurring in the second quarter of each year, followed by a dip in the third quarter. This pattern is likely due to various factors, such as changes in consumer spending and production schedules. Multiplicative decomposition model may be more appropriate, as it accounts for changes in both the level and the variability of the data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"GDP Growth Rate\", main = \"U.S GDP Growth Rate: 2010 - 2021\")\ndecompose = decompose(myts,\"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\n#adjusted decomposition\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the multiplicative time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the multiplicative time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original plot, the adjusted seasonal component tends to have more fluctuation, and the model is more variable than the original plot, where the plot changes over time but the trend stays the same.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S GDP Growth Rate: 2010 - 2021\")\n\n\n\n\n\n\n\n\n\nCode\n#montly data\nmean_data <- gdp %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(value))\n\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\n#Lag plot for month\nts_lags(month,lags = c(1, 4, 7, 10) )\n\n\n\n\n\n\n\n\n\nThe lag plot shows that there is a cluster in the middle, and the monthly lag plot shows that there is no autocorrelation.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"gdp\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"GDP Growth Rate 2010 - 2022 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe three plots show the same data series of GDP growth rate from 2010 to 2022, but each plot has a different moving average smoothing applied to it. The first plot shows a 4-month moving average, the second plot shows a 1-year moving average, and the third plot shows a 3-year moving average.\nLooking at the three plots, we can see that the 4-month moving average plot has a lot of fluctuations, and it follows the ups and downs of the original data series more closely. The 1-year moving average plot has less fluctuations compared to the 4-month moving average plot, and it provides a smoother trend of the data series. The 3-year moving average plot has even less fluctuations and a much smoother trend than the previous two plots.\nThe choice of moving average window size depends on the analyst’s preference and the objective of the analysis. Shorter window sizes like the 4-month moving average can provide more detailed insights into the data series, but they may also be more susceptible to noise and fluctuations. Longer window sizes like the 3-year moving average can provide a more stable and robust trend but may smooth out important details in the data series. As the moving average increases, GDP Growth Rtae tend to have no trend, it seem to be stable.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for month data\nggAcf(month)+ggtitle(\"ACF Plot for GDP Growth Rate: 2010 - 2022\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for month data\nggPacf(month)+ggtitle(\"PACF Plot for GDP Growth Rate: 2010 - 2022\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nThe above autocorrelation plots show that the series doesn’t change with the seasons, which indicates that there series is stationary. This is verified was checked using the Augmented Dickey-Fuller Test and the result of the test says that series is stationary because the p value is less than 0.05."
  },
  {
    "objectID": "eda.html#interest-rate",
    "href": "eda.html#interest-rate",
    "title": "Exploratory Data Analysis",
    "section": "Interest Rate",
    "text": "Interest Rate\n\nTime Series Plot\n\n\nCode\n#import the data\ninterest_data <- read.csv(\"DATA/RAW DATA/interest-rate.csv\")\n\n#change date format\ninterest_data$Date <- as.Date(interest_data$Date , \"%m/%d/%Y\")\n\n#export the cleaned data\ninterest_clean_data <- interest_data\nwrite.csv(interest_clean_data, \"DATA/CLEANED DATA/interest_rate_clean_data.csv\", row.names=FALSE)\n\n#convert to ts data\nmyts<-ts(interest_data$value,frequency=12,start=c(2010/1/1))\ninterest_ts <- myts\n#export the data\nwrite.csv(interest_ts, \"DATA/CLEANED DATA/interest_rate_ts_data.csv\", row.names=FALSE)\n\n#plot interest rate \nfig <- plot_ly(interest_data, x = ~Date, y = ~value, type = 'scatter', mode = 'lines',line = list(color='rgb(176,224,230)'))\nfig <- fig %>% layout(title = \"U.S Interest Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Interest Rate\"))\nfig\n\n\n\n\n\n\nThe interest rate in the United States has exhibited fluctuations from 2010 to 2022. In the years following the 2008 global financial crisis, the interest rate was very low. The interest rate in the US was very low and stable from 2010 to 2015. During this period, the Federal Reserve implemented several monetary policy measures, such as quantitative easing and forward guidance, in order to stimulate the economy and support economic recovery after the global financial crisis.\nStarting in 2015, the Federal Reserve began a gradual process of raising interest rates as the US economy continued to improve. This process of increasing interest rates was driven by a combination of factors such as low unemployment rates, a steady increase in GDP, and the need to prevent inflation from rising too quickly.\nHowever, as global economic conditions became more uncertain, the Federal Reserve began to pause its process of increasing interest rates. The US-China trade war and concerns about the potential impact of Brexit led to a more cautious approach from the Federal Reserve. In 2019, the Federal Reserve lowered interest rates three times in response to these external factors.\nIn 2020, the COVID-19 pandemic caused a major shock to the global economy, leading the Federal Reserve to take unprecedented measures to support the US economy. The Federal Reserve lowered interest rates to near-zero levels, implemented quantitative easing, and established several lending facilities to support businesses and households.\nThere is no clear evidence of a relationship between the variability of the series and its level, which suggests that an additive model might be more appropriate. Additionally, an additive model can be useful when the trend is relatively stable and the amplitude of seasonal fluctuations remains constant over time.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Interest Rate: January 2010 - March 2023\")\ndecompose = decompose(myts,\"additive\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows no seasonality in the model, but the adjusted trend component has a stable trend through time.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#monthly data\nmean_data <- interest_data %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(value))\n\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n\n#Lag plot for monthly data\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to March 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with strong positive autocorrelation. One observation and the next have a significant link, making such processes remarkably non-random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. A closer look at the previous graph indicates that there are more dots on the diagonal line at 45 degrees. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the strong seasonality of the data.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"interest_data\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Interest Rate January 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Interest Rate is displayed for the time period between January 2010 and March 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. The trend for Interest Rate isn’t stable, there is fluctuation in the trend but it seems that there is upward trend from 2020.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for monthly data\nggAcf(month)+ggtitle(\"ACF Plot for Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(month)+ggtitle(\"PACF Plot for Interest Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -1.3832, Lag order = 5, p-value = 0.8336\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.97697 -0.33564  0.00804  0.23469  1.46755 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)\n(Intercept) -18.300672  20.233604  -0.904    0.367\ntime(myts)    0.009339   0.010034   0.931    0.353\n\nResidual standard error: 0.4839 on 157 degrees of freedom\nMultiple R-squared:  0.005488,  Adjusted R-squared:  -0.0008465 \nF-statistic: 0.8664 on 1 and 157 DF,  p-value: 0.3534\n\n\n\n\n\n\nCode\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Interest Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.00933. With a standard error of 0.010034, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(18.300672)-(0.00933.)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda.html#inflation-rate",
    "href": "eda.html#inflation-rate",
    "title": "Exploratory Data Analysis",
    "section": "Inflation Rate",
    "text": "Inflation Rate\n\nTime Series Plot\n\n\nCode\n#import the data\ninflation_rate <- read.csv(\"DATA/RAW DATA/inflation-rate.csv\")\n\n#cleaning the data\n#remove unwanted columns\ninflation_rate_clean <- subset(inflation_rate, select = -c(1,HALF1,HALF2))\n\n#convert the data to time series data\ninflation_data_ts <- ts(as.vector(t(as.matrix(inflation_rate_clean))), start=c(2010,1), end=c(2023,2), frequency=12)\n\n#export the data\nwrite.csv(inflation_rate_clean, \"DATA/CLEANED DATA/inflation_rate_clean_data.csv\", row.names=FALSE)\nwrite.csv(inflation_data_ts, \"DATA/CLEANED DATA/inflation_rate_ts_data.csv\", row.names=FALSE)\n\n#plot inflation rate \nfig <- autoplot(inflation_data_ts, ylab = \"Inflation Rate\", color=\"#DB7093\")+ggtitle(\"U.S Inflation Rate: January 2010 - February 2023\")+theme_bw()\nggplotly(fig)\n\n\n\n\n\n\nThe inflation rate in the United States has varied from year to year since 2010. From 2010 to 2015, the inflation rate generally remained below 2% per year, with some slight fluctuations. In 2016, it started to rise gradually and continued to increase until it reached a peak of 6.6% in June 2022. Since then, it has slightly decreased and as of February 2023. The COVID-19 pandemic has played a significant role in driving up inflation in the United States, as supply chain disruptions and increased demand have led to higher prices for goods and services. The Federal Reserve has taken steps to address inflation, including raising interest rates and reducing asset purchases, in order to keep it under control.\nBased on the plot of the inflation rate in the USA from 2010 to Feb 2023, it appears that there is a clear upward trend, and some level of seasonality as well. Therefore, it would be appropriate to use a multiplicative decomposition method for this time series data. A multiplicative model will allow us to separate the overall trend from the seasonal variations in a way that is appropriate for this type of data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#decomposition\norginial_plot <- autoplot(inflation_data_ts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Inflation Rate: January 2010 - Feb 2023\")\ndecompose = decompose(inflation_data_ts,\"multiplicative\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- inflation_data_ts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- inflation_data_ts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows tend to show upward trend in the model, but the adjusted trend component has a stable trend through time with some fluctuation.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(inflation_data_ts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#Lag plot for monthly data\nts_lags(inflation_data_ts)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to Feb 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with positive autocorrelation. One observation and the next have a significant link, making such processes remarkably random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the seasonality of the data.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 20233 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(inflation_data_ts, series=\"inflation_rate_clean\") +\n  autolayer(ma(inflation_data_ts,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Inflation Rate January 2010 - Feb 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Inflation Rate is displayed for the time period between January 2010 and Feb 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. As the moving average increases we can notive that the trend for Inflation Rate isn’t stable is towards upward.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for monthly data\nggAcf(inflation_data_ts)+ggtitle(\"ACF Plot for Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(inflation_data_ts)+ggtitle(\"PACF Plot for Inflation Rate: January 2010 - Feb 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(inflation_data_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  inflation_data_ts\nDickey-Fuller = -1.7207, Lag order = 5, p-value = 0.6928\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it. The p value obtained from ADF test is greater than 0.05, which indicates taht the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(inflation_data_ts~time(inflation_data_ts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = inflation_data_ts ~ time(inflation_data_ts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1136 -0.6549 -0.1224  0.4632  2.8301 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             -474.68145   43.32414  -10.96   <2e-16 ***\ntime(inflation_data_ts)    0.23655    0.02148   11.01   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.026 on 156 degrees of freedom\nMultiple R-squared:  0.4373,    Adjusted R-squared:  0.4337 \nF-statistic: 121.2 on 1 and 156 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(inflation_data_ts, 48, main=\"Original Data: Inflation Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(inflation_data_ts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, 0.23655 With a standard error of 0.02148, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(474.68145)-(0.23655)t\\]\nFrom the above graph we can say that there is correlation in the original plot, but in the detrended plot the correlation is reduced but there is still high correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "eda.html#unemployment-rate",
    "href": "eda.html#unemployment-rate",
    "title": "Exploratory Data Analysis",
    "section": "Unemployment Rate",
    "text": "Unemployment Rate\n\nTime Series Plot\n\n\nCode\n#import the data\nunemployment_rate <- read.csv(\"DATA/RAW DATA/unemployment-rate.csv\")\n\n#change date format\nunemployment_rate$Date <- as.Date(unemployment_rate$Date , \"%m/%d/%Y\")\n\n#convert the data to ts data\nmyts<-ts(unemployment_rate$Value,frequency=12,start=c(2010/1/1))\nunemployment_rate_ts <- myts\n# export the data\nwrite.csv(unemployment_rate, \"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\", row.names=FALSE)\nwrite.csv(unemployment_rate_ts, \"DATA/CLEANED DATA/unemployment_rate_ts_data.csv\", row.names=FALSE)\n\n\n#plot unemployment rate \nfig <- plot_ly(unemployment_rate, x = ~Date, y = ~Value, type = 'scatter', mode = 'lines',line = list(color = 'rgb(255,215,0)'))\nfig <- fig %>% layout(title = \"U.S Unemployment Rate: January 2010 - March 2023\",xaxis = list(title = \"Time\"),yaxis = list(title =\"Unemployment Rate\"))\nfig\n\n\n\n\n\n\nThe unemployment rate in the United States has seen significant fluctuations since 2010, with various economic factors contributing to changes in the rate over the years. The data from the Federal Reserve Economic Data (FRED) series shows that the unemployment rate peaked at 9.9% in 2010, following the 2008 financial crisis. However, it has steadily declined over the years and currently stands at 3.9% as of February 2023.\nThe first half of the 2010s saw a slow but steady decline in the unemployment rate, dropping from the 9.9% peak in 2010 to 5.3% by 2015. The latter half of the decade saw even further improvements, with the rate hitting a low of 3.5% in September 2019. However, the onset of the COVID-19 pandemic in early 2020 led to a sharp increase in unemployment, with the rate skyrocketing to 14.8% in April of that year.\nSince then, the unemployment rate has been slowly but steadily improving as the economy recovers from the pandemic-induced recession. By the end of 2021, the rate had fallen to 4.2% and has continued to decline into 2022 and 2023. However, it is worth noting that some industries and sectors are still struggling to recover from the pandemic, and some individuals have not yet returned to the labor force, which could impact the overall unemployment rate.\nOverall, the unemployment rate in the United States has undergone significant fluctuations over the past decade, with various economic and social factors contributing to the changes.\nFrom the graph, it appears that the magnitude of the seasonal fluctuations in the unemployment rate has remained relatively constant over time, while the overall trend has shown both increasing and decreasing phases. Therefore, an additive decomposition method could be appropriate for analyzing the time series data.\n\n\nDecomposed Time Series\n\nDecomposition PlotAdjusted Decomposition Plot\n\n\n\n\nCode\n#decomposition\norginial_plot <- autoplot(myts,xlab =\"Year\", ylab = \"Interest Rate\", main = \"U.S Unemployment Rate: January 2010 - March 2023\")\ndecompose = decompose(myts,\"additive\")\nautoplot(decompose)\n\n\n\n\n\n\n\n\n\nCode\ntrendadj <- myts/decompose$trend\ndecompose_adjtrend_plot <- autoplot(trendadj,ylab='seasonal') +ggtitle('Adjusted trend component in the additive time series model')\nseasonaladj <- myts/decompose$seasonal\ndecompose_adjseasonal_plot <- autoplot(seasonaladj,ylab='seasonal') +ggtitle('Adjusted seasonal component in the additive time series model')\ngrid.arrange(orginial_plot, decompose_adjtrend_plot,decompose_adjseasonal_plot, nrow=3)\n\n\n\n\n\n\n\n\nWhen compared to the original figure, the corrected seasonal component shows some seasonality in the model, but the adjusted trend component has a stable trend through time with a high increase due to pandemic, but there the trend dropped after few months.\n\n\nLag Plots\n\nDaily Time LagsMonthly Time Lags\n\n\n\n\nCode\n#Lag plots \ngglagplot(myts, do.lines=FALSE, lags=1)+xlab(\"Lag 1\")+ylab(\"Yi\")+ggtitle(\"Lag Plot for U.S Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#monthly data\nmean_data <- unemployment_rate %>% \n  mutate(month = month(Date), year = year(Date)) %>% \n  group_by(year, month) %>% \n  summarize(mean_value = mean(Value))\nmonth<-ts(mean_data$mean_value,star=decimal_date(as.Date(\"2010-01-01\",format = \"%Y-%m-%d\")),frequency = 12)\n#Lag plot for monthly data\nts_lags(month)\n\n\n\n\n\n\n\n\n\nThere should be a strong relationship between the series and the pertinent lag from January 2010 to March 2023 because there is a positive correlation and an inclination angle of 45 degrees in the lag plot. This is the characteristic lag plot of a process with strong positive autocorrelation. One observation and the next have a significant link, making such processes remarkably non-random. Investigating seasonality also involves graphing observations over a larger range of time intervals, or the lags. To make the time series data easier to understand and create graphs with more clarity, the time series data is combined with monthly data using the mean function. A closer look at the previous graph indicates that there are more dots on the diagonal line at 45 degrees. The second graph displays the variable’s monthly variation along the vertical axis. The lines link the points in the order of time. There is a correlation and it is significantly positive, supporting the strong seasonality of the data.\n\n\nMoving Average\n\n4 Month MA1 Year MA3 Year MA5 Year MA\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,5), series=\"4 Month MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (4 Month Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"4 Month MA\"=\"red\"),\n                      breaks=c(\"Data\",\"4 Month MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,13), series=\"1 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (1 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"1 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"1 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,37), series=\"3 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (3 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"3 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"3 Year MA\"))\nma\n\n\n\n\n\n\n\n\n\nCode\n#SMA Smoothing \nma <- autoplot(month, series=\"unemployment_rate\") +\n  autolayer(ma(month,61), series=\"5 Year MA\") +\n  xlab(\"Year\") + ylab(\"GWh\") +\n  ggtitle(\"Unemployment Rate January 2010 - March 2023 (5 Year Moving Average)\") +\n  scale_colour_manual(values=c(\"Data\"=\"grey50\",\"5 Year MA\"=\"red\"),\n                      breaks=c(\"Data\",\"5 Year MA\"))\nma\n\n\n\n\n\n\n\n\nThe Unemployment Rate is displayed for the time period between January 2010 and March 2023 in the four plots above, which have been smoothed using 4-month, 1-year, 3-year, and 5-year moving averages. We can observe from the graphs that the moving average values have been rising over time. Given that it captures the short-term differences in stock prices, the 4-month MA plot exhibits a great deal of volatility, which is to be expected. The 1-year, 3-year, and 5-year MA plots, on the other hand, tame the oscillations and reveal the general direction of stock prices. We can see that the 5-year MA plot, which considers a longer time period than the other plots, shows a trend that is smoother. The 3-year MA plot similarly shows a fairly smooth trend, but unlike the 5-year MA plot, it also shows shorter-term variability. Even more susceptible to short-term changes in stock prices is the 1-year MA plot. The trend for Unemployment Rate was downward from 2010 to 2019, but there is increase in the moving average due to the increase in unemployment rate in US during pandemic.\n\n\nAutocorrelation Time Series\n\nACFPACFADF Test\n\n\n\n\nCode\n#ACF plots for monthly data\nggAcf(month)+ggtitle(\"ACF Plot for Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n#PACF plots for monthly data\nggPacf(month)+ggtitle(\"PACF Plot for Unemployment Rate: January 2010 - March 2023\")\n\n\n\n\n\n\n\n\n\nCode\n# ADF Test\ntseries::adf.test(month)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  month\nDickey-Fuller = -2.7517, Lag order = 5, p-value = 0.2629\nalternative hypothesis: stationary\n\n\n\n\n\nThe autocorrelation function plot, which is the acf graph for monthly data, clearly shows autocorrelation in lag. The series appears to be seasonal, according to the lag plots and autocorrelation plots displayed above, proving that it is not stable. The Augmented Dickey-Fuller Test, which reveals that the series is not stationary if the p value is more than 0.05, was also used to validate it. The p value obtained from ADF test is greater than 0.05, which indicates taht the series is not stationary.\n\n\nDetrend and Differenced Time Series\n\nLinear Fitting ModelACF Plot\n\n\n\n\nCode\nfit = lm(myts~time(myts), na.action=NULL) \nsummary(fit) \n\n\n\nCall:\nlm(formula = myts ~ time(myts), na.action = NULL)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.5761 -1.2396 -0.2468  0.7124 10.0644 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 785.15846   72.32427   10.86   <2e-16 ***\ntime(myts)   -0.38635    0.03587  -10.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.713 on 156 degrees of freedom\nMultiple R-squared:  0.4266,    Adjusted R-squared:  0.4229 \nF-statistic:   116 on 1 and 156 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\nCode\n# plot ACFs\nplot1 <- ggAcf(myts, 48, main=\"Original Data: Unemployment Rate\")\nplot2 <- ggAcf(resid(fit), 48, main=\"Detrended data\") \nplot3 <- ggAcf(diff(myts), 48, main=\"First differenced data\")\ngrid.arrange(plot1, plot2, plot3,nrow=3)\n\n\n\n\n\n\n\n\nThe estimated slope coefficient β1, -0.38635 With a standard error of 0.03587, yielding a significant estimated increase of stock price is very less yearly. Equation of the fit for stationary process: \\[\\hat{y}_{t} = x_{t}+(785.15846)-(0.38635)t\\]\nFrom the above graph we can say that there is high correlation in the original plot, but in the detrended plot the correlation is reduced but there is still correlation in the detrended data.But when the first order difference is applied the high correlation is removed but there is seasonal correlation.\nAs depicted in the above figure, the series is now stationary and ready for future study."
  },
  {
    "objectID": "arma.html",
    "href": "arma.html",
    "title": "Modeling Time Series Data",
    "section": "",
    "text": "When working with time series data, it is common to start with autoregressive (AR), moving average (MA), or autoregressive moving average (ARMA) models. Additionally, autoregressive integrated moving average (ARIMA) and seasonal autoregressive integrated moving average (SARIMA) models are widely used for better understanding the data and forecasting future values based on historical data. However, before using these models, it is crucial to check for stationarity in the time series data, which means that the mean and variance should not change over time. Non-stationarity is often caused by trends, where the values slowly increase or decrease over time.\nTo test for stationarity, an autocorrelation function (ACF) plot can be used to check for correlations between the time series and its lagged version. If there is a significant correlation, then the data is likely non-stationary. In such cases, taking the first or second differences of the data can help remove any trends or seasonality and make the data stationary.\nIn the EDA tab of our project, we have executed an ACF plot to test for stationarity and made the necessary data transformations to make the data stationary. The ACF and partial autocorrelation (PACF) plots for the stationary data can help identify the appropriate parameters for our ARIMA or SARIMA models.\n\nUnitedHealth Group Incorporated\n\nDifferentiated Time Series Plot\n\nACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/uhn_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$UNH.Adjusted,frequency=365,start=c(2015,1,1)) \n#First order differentiation\ndf1 <- diff(myts)\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -14.363, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 1,2,3 (PACF Plot) q = 1,2,3 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*9),nrow=9) #nrow = 3x3x1\n\n\nfor (p in 2:4)# p=1,2,3 :3\n{\n  for(q in 2:4)# q=1,2,3 :3\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n12180.93\n12203.44\n12180.95\n\n\n1\n1\n2\n12177.78\n12205.91\n12177.80\n\n\n1\n1\n3\n12166.17\n12199.93\n12166.21\n\n\n2\n1\n1\n12178.80\n12206.93\n12178.83\n\n\n2\n1\n2\n12137.16\n12170.92\n12137.20\n\n\n2\n1\n3\n12164.39\n12203.78\n12164.45\n\n\n3\n1\n1\n12164.79\n12198.55\n12164.83\n\n\n3\n1\n2\n12165.59\n12204.98\n12165.65\n\n\n3\n1\n3\n12136.74\n12181.75\n12136.81\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(3,1,4) with drift \n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3      ma4   drift\n      -0.7674  0.7154  0.8277  0.7094  -0.7056  -0.8305  -0.0619  0.1943\ns.e.   0.0356  0.0392  0.0433  0.0414   0.0371   0.0483   0.0275  0.0513\n\nsigma^2 = 21.46:  log likelihood = -6053.86\nAIC=12125.72   AICc=12125.81   BIC=12176.36\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (3,1,3) model, while the ARIMA (2,1,2) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (3,1,4) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2Model 3 PlotModel 3\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,3,1,3))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[205:240], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3  constant\n      -0.7255  0.7967  0.9094  0.6476  -0.7888  -0.8587    0.2062\ns.e.   0.0269  0.0144  0.0266  0.0336   0.0186   0.0343    0.0245\n\nsigma^2 estimated as 21.5:  log likelihood = -6060.37,  aic = 12136.74\n\n$degrees_of_freedom\n[1] 2045\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.7255 0.0269 -26.9794       0\nar2        0.7967 0.0144  55.2119       0\nar3        0.9094 0.0266  34.1665       0\nma1        0.6476 0.0336  19.2509       0\nma2       -0.7888 0.0186 -42.5119       0\nma3       -0.8587 0.0343 -25.0447       0\nconstant   0.2062 0.0245   8.4297       0\n\n$AIC\n[1] 5.914588\n\n$AICc\n[1] 5.914615\n\n$BIC\n[1] 5.936524\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,2,1,2))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[119:152], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1      ar2     ma1     ma2  constant\n      -1.7221  -0.9158  1.6499  0.8602    0.1855\ns.e.   0.0268   0.0266  0.0336  0.0346    0.0989\n\nsigma^2 estimated as 21.56:  log likelihood = -6062.58,  aic = 12137.16\n\n$degrees_of_freedom\n[1] 2047\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -1.7221 0.0268 -64.3094  0.0000\nar2       -0.9158 0.0266 -34.4209  0.0000\nma1        1.6499 0.0336  49.1130  0.0000\nma2        0.8602 0.0346  24.8624  0.0000\nconstant   0.1855 0.0989   1.8750  0.0609\n\n$AIC\n[1] 5.914794\n\n$AICc\n[1] 5.914808\n\n$BIC\n[1] 5.931246\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,3,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[102:138], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3     ma1      ma2      ma3      ma4  constant\n      -0.7674  0.7154  0.8277  0.7094  -0.7056  -0.8305  -0.0619    0.1943\ns.e.   0.0356  0.0392  0.0433  0.0414   0.0371   0.0483   0.0275    0.0513\n\nsigma^2 estimated as 21.38:  log likelihood = -6053.86,  aic = 12125.72\n\n$degrees_of_freedom\n[1] 2044\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.7674 0.0356 -21.5296  0.0000\nar2        0.7154 0.0392  18.2510  0.0000\nar3        0.8277 0.0433  19.0972  0.0000\nma1        0.7094 0.0414  17.1536  0.0000\nma2       -0.7056 0.0371 -19.0057  0.0000\nma3       -0.8305 0.0483 -17.1880  0.0000\nma4       -0.0619 0.0275  -2.2509  0.0245\nconstant   0.1943 0.0513   3.7874  0.0002\n\n$AIC\n[1] 5.90922\n\n$AICc\n[1] 5.909255\n\n$BIC\n[1] 5.933898\n\n\n\n\n\nTo determine which model diagnosis is the best, we need to consider some key factors such as the coefficient estimates and standard errors, AIC, BIC, p-values, and the log-likelihood. Comparing the three models, model 2 is the best because it has the lowest AIC and BIC values, which indicate better goodness of fit. Also, all the coefficient estimates in the model are significant, as evidenced by the p-values being equal to zero, which implies that there is a strong correlation between the variables. The t-values are high, indicating that the estimates are highly reliable.\nTherefore, based on the AIC and BIC values, the significance of the coefficient estimates, and the reliability of the estimates, we can conclude that model 2 is the best model diagnosis. Therefore, (2,1,2) is the best model for this time series.\nThe best model identified is ARIMA(2,1,2). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1, approaching 3. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests some normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[(1-\\phi_1B-\\phi_2B^2)(1-B)(Y_t-\\mu) = (1+\\theta_1B+\\theta_2B^2)\\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(2,1,2),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"UNH Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 2,1,2, main='UNH Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2020, 229) \nEnd = c(2021, 45) \nFrequency = 365 \n  [1] 476.3632 474.6717 476.3248 475.7017 475.9354 476.7782 475.7875 477.3964\n  [9] 476.2077 477.4560 477.0696 477.2665 477.9559 477.2630 478.4996 477.6793\n [17] 478.6341 478.4157 478.5920 479.1631 478.6929 479.6543 479.1039 479.8459\n [25] 479.7468 479.9126 480.3925 480.0889 480.8469 480.4943 481.0820 481.0674\n [33] 481.2289 481.6388 481.4597 482.0674 481.8596 482.3356 482.3808 482.5417\n [41] 482.8979 482.8118 483.3085 483.2066 483.6018 483.6892 483.8515 484.1667\n [49] 484.1499 484.5648 484.5404 484.8771 484.9942 485.1588 485.4428 485.4777\n [57] 485.8322 485.8644 486.1589 486.2969 486.4643 486.7243 486.7978 487.1078\n [65] 487.1814 487.4454 487.5980 487.7681 488.0100 488.1122 488.3893 488.4932\n [73] 488.7352 488.8980 489.0707 489.2989 489.4224 489.6753 489.8013 490.0274\n [81] 490.1973 490.3723 490.5900 490.7295 490.9646 491.1067 491.3214 491.4962\n [89] 491.6732 491.8829 492.0343 492.2562 492.4101 492.6165 492.7948 492.9734\n [97] 493.1772 493.3373 493.5496 493.7120 493.9125 494.0931 494.2732 494.4724\n[105] 494.6391 494.8442 495.0130 495.2092 495.3914 495.5726 495.7684 495.9399\n[113] 496.1398 496.3131 496.5063 496.6896 496.8717 497.0649 497.2400 497.4361\n[121] 497.6127 497.8037 497.9878 498.1705 498.3618 498.5397 498.7329 498.9119\n[129] 499.1013 499.2859 499.4693 499.6591 499.8389 500.0301 500.2109 500.3991\n[137] 500.5840 500.7678 500.9566 501.1379 501.3275 501.5096 501.6970 501.8821\n[145] 502.0663 502.2542 502.4366 502.6251 502.8081 502.9950 503.1803 503.3647\n[153] 503.5520 503.7352 503.9229 504.1066 504.2930 504.4784 504.6631 504.8499\n[161] 505.0337 505.2208 505.4049 505.5911 505.7765 505.9614 506.1479 506.3321\n[169] 506.5187 506.7033 506.8892 507.0747 507.2597 507.4459 507.6304 507.8168\n[177] 508.0015 508.1873 508.3728 508.5579 508.7439 508.9287\n\n$se\nTime Series:\nStart = c(2020, 229) \nEnd = c(2021, 45) \nFrequency = 365 \n  [1]  4.643563  6.334419  7.844622  8.986985 10.055306 11.023054 11.865471\n  [8] 12.726442 13.450654 14.211419 14.880551 15.546177 16.186848 16.777696\n [15] 17.388989 17.933998 18.504212 19.027466 19.550854 20.062690 20.545959\n [22] 21.043064 21.500677 21.974156 22.419324 22.864299 23.302533 23.722240\n [29] 24.150825 24.553846 24.966692 25.361140 25.754802 26.143966 26.520329\n [36] 26.902230 27.266779 27.637293 27.995278 28.352067 28.705565 29.049850\n [43] 29.397457 29.732890 30.071811 30.401895 30.730561 31.056651 31.375908\n [50] 31.697021 32.009365 32.323598 32.631413 32.937722 33.241916 33.540913\n [57] 33.840798 34.134220 34.428502 34.717995 35.005982 35.292168 35.574315\n [64] 35.856717 36.134250 36.411985 36.686056 36.958671 37.229713 37.497553\n [71] 37.765238 38.029179 38.292905 38.553762 38.813237 39.071316 39.326807\n [78] 39.581884 39.834023 40.085693 40.335061 40.583137 40.829956 41.074647\n [85] 41.318767 41.560534 41.801682 42.040944 42.279006 42.515926 42.751067\n [92] 42.985548 43.218107 43.449967 43.680249 43.909423 44.137552 44.364170\n [99] 44.590083 44.814398 45.037978 45.260215 45.481428 45.701681 45.920632\n[106] 46.138864 46.355742 46.571880 46.786854 47.000883 47.214026 47.426032\n[113] 47.637325 47.847453 48.056851 48.265227 48.472730 48.679412 48.885091\n[120] 49.090073 49.294038 49.497295 49.699642 49.901183 50.101961 50.301843\n[127] 50.501052 50.699361 50.896991 51.093803 51.289867 51.485220 51.679767\n[134] 51.873669 52.066765 52.259214 52.450919 52.641930 52.832277 53.021892\n[141] 53.210892 53.399163 53.586819 53.773795 53.960125 54.145834 54.330871\n[148] 54.515325 54.699114 54.882320 55.064899 55.246876 55.428270 55.609046\n[155] 55.789268 55.968879 56.147939 56.326417 56.504332 56.681699 56.858494\n[162] 57.034764 57.210468 57.385652 57.560293 57.734406 57.908004 58.081069\n[169] 58.253637 58.425679 58.597227 58.768269 58.938814 59.108874 59.278435\n[176] 59.447524 59.616123 59.784255 59.951911 60.119098 60.285827 60.452089\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,1,2)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(2,1,2)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\n\nApple\n\nDifferentiated Time Series Plot\n\nACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/apple_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$AAPL.Adjusted,frequency=365,start=c(2015,1,1))  \n#First order differentiation\ndf1 <- diff(myts)\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -12.332, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 1 (PACF Plot) q = 1 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*4),nrow=4) #nrow = 2x2x1\n\n\nfor (p in 1:2)# p=0,1 :2\n{\n  for(q in 1:2)# q=0,1 :2\n  {\n    for(d in 1)# d=2 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n8169.286\n8180.539\n8169.292\n\n\n0\n1\n1\n8164.414\n8181.294\n8164.426\n\n\n1\n1\n0\n8164.789\n8181.669\n8164.801\n\n\n1\n1\n1\n8164.583\n8187.089\n8164.603\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1   drift\n      -0.0596  0.0599\ns.e.   0.0227  0.0367\n\nsigma^2 = 3.123:  log likelihood = -4079.21\nAIC=8164.41   AICc=8164.43   BIC=8181.29\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value corresponds to an ARIMA (0,1,1) model, while the ARIMA (0,1,0) model has the lowest BIC value. Additionally, the auto.arima function in R suggests an ARIMA (0,1,1) model as the best fit for the data. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[12:42], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -0.0596    0.0599\ns.e.   0.0227    0.0367\n\nsigma^2 estimated as 3.12:  log likelihood = -4079.21,  aic = 8164.41\n\n$degrees_of_freedom\n[1] 2050\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -0.0596 0.0227 -2.6242  0.0088\nconstant   0.0599 0.0367  1.6326  0.1027\n\n$AIC\n[1] 3.978759\n\n$AICc\n[1] 3.978762\n\n$BIC\n[1] 3.986985\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts,0,1,0))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0599\ns.e.    0.0391\n\nsigma^2 estimated as 3.131:  log likelihood = -4082.64,  aic = 8169.29\n\n$degrees_of_freedom\n[1] 2051\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0599 0.0391  1.5326  0.1255\n\n$AIC\n[1] 3.981134\n\n$AICc\n[1] 3.981135\n\n$BIC\n[1] 3.986618\n\n\n\n\n\nThe two models is better based on the information provided. Both models have very small difference in AIC ,AICc and BIC values, indicating that they fit the data similarly well. But if we consider the both the sitution, it indicates that Model 1 is a better model because, the values are lower than model 2 values and the auto.arima function also suggest model 1 is best. So, (0,1,1) is the best model for this time series.\nThe best model identified is ARIMA(0,1,1). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1, approaching 6 for the recent years. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests some normality in the residuals. Additionally, the p-values for the Ljung-Box test are greater than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[Y_t = Y_{t-1} + \\theta_1 (e_{t-1}) + e_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Apple Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,1, main='Apple Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2020, 229) \nEnd = c(2021, 45) \nFrequency = 365 \n  [1] 147.5003 147.5602 147.6201 147.6799 147.7398 147.7997 147.8596 147.9194\n  [9] 147.9793 148.0392 148.0990 148.1589 148.2188 148.2787 148.3385 148.3984\n [17] 148.4583 148.5182 148.5780 148.6379 148.6978 148.7576 148.8175 148.8774\n [25] 148.9373 148.9971 149.0570 149.1169 149.1768 149.2366 149.2965 149.3564\n [33] 149.4162 149.4761 149.5360 149.5959 149.6557 149.7156 149.7755 149.8354\n [41] 149.8952 149.9551 150.0150 150.0749 150.1347 150.1946 150.2545 150.3143\n [49] 150.3742 150.4341 150.4940 150.5538 150.6137 150.6736 150.7335 150.7933\n [57] 150.8532 150.9131 150.9729 151.0328 151.0927 151.1526 151.2124 151.2723\n [65] 151.3322 151.3921 151.4519 151.5118 151.5717 151.6316 151.6914 151.7513\n [73] 151.8112 151.8710 151.9309 151.9908 152.0507 152.1105 152.1704 152.2303\n [81] 152.2902 152.3500 152.4099 152.4698 152.5296 152.5895 152.6494 152.7093\n [89] 152.7691 152.8290 152.8889 152.9488 153.0086 153.0685 153.1284 153.1882\n [97] 153.2481 153.3080 153.3679 153.4277 153.4876 153.5475 153.6074 153.6672\n[105] 153.7271 153.7870 153.8469 153.9067 153.9666 154.0265 154.0863 154.1462\n[113] 154.2061 154.2660 154.3258 154.3857 154.4456 154.5055 154.5653 154.6252\n[121] 154.6851 154.7449 154.8048 154.8647 154.9246 154.9844 155.0443 155.1042\n[129] 155.1641 155.2239 155.2838 155.3437 155.4036 155.4634 155.5233 155.5832\n[137] 155.6430 155.7029 155.7628 155.8227 155.8825 155.9424 156.0023 156.0622\n[145] 156.1220 156.1819 156.2418 156.3016 156.3615 156.4214 156.4813 156.5411\n[153] 156.6010 156.6609 156.7208 156.7806 156.8405 156.9004 156.9603 157.0201\n[161] 157.0800 157.1399 157.1997 157.2596 157.3195 157.3794 157.4392 157.4991\n[169] 157.5590 157.6189 157.6787 157.7386 157.7985 157.8583 157.9182 157.9781\n[177] 158.0380 158.0978 158.1577 158.2176 158.2775 158.3373\n\n$se\nTime Series:\nStart = c(2020, 229) \nEnd = c(2021, 45) \nFrequency = 365 \n  [1]  1.766461  2.424905  2.939378  3.376347  3.762911  4.113305  4.436108\n  [8]  4.736965  5.019822  5.287570  5.542398  5.786014  6.019778  6.244799\n [15]  6.461988  6.672111  6.875816  7.073657  7.266113  7.453602  7.636489\n [22]  7.815097  7.989713  8.160594  8.327970  8.492047  8.653013  8.811039\n [29]  8.966281  9.118880  9.268967  9.416662  9.562077  9.705312  9.846465\n [36]  9.985622 10.122866 10.258275 10.391919 10.523866 10.654179 10.782918\n [43] 10.910137 11.035890 11.160226 11.283192 11.404833 11.525189 11.644302\n [50] 11.762208 11.878945 11.994545 12.109041 12.222466 12.334847 12.446213\n [57] 12.556592 12.666009 12.774489 12.882055 12.988730 13.094537 13.199495\n [64] 13.303626 13.406947 13.509478 13.611238 13.712241 13.812507 13.912050\n [71] 14.010885 14.109028 14.206494 14.303295 14.399445 14.494958 14.589845\n [78] 14.684119 14.777792 14.870875 14.963378 15.055314 15.146691 15.237520\n [85] 15.327812 15.417574 15.506817 15.595549 15.683779 15.771515 15.858767\n [92] 15.945540 16.031844 16.117686 16.203074 16.288013 16.372512 16.456577\n [99] 16.540215 16.623432 16.706234 16.788629 16.870620 16.952215 17.033420\n[106] 17.114239 17.194678 17.274742 17.354437 17.433768 17.512740 17.591357\n[113] 17.669624 17.747546 17.825128 17.902373 17.979286 18.055872 18.132134\n[120] 18.208077 18.283704 18.359020 18.434028 18.508733 18.583136 18.657244\n[127] 18.731058 18.804582 18.877820 18.950775 19.023450 19.095848 19.167973\n[134] 19.239828 19.311415 19.382738 19.453799 19.524602 19.595149 19.665443\n[141] 19.735486 19.805282 19.874833 19.944141 20.013209 20.082040 20.150635\n[148] 20.218998 20.287130 20.355035 20.422713 20.490168 20.557402 20.624416\n[155] 20.691214 20.757796 20.824166 20.890325 20.956274 21.022017 21.087556\n[162] 21.152891 21.218024 21.282959 21.347696 21.412237 21.476584 21.540739\n[169] 21.604704 21.668480 21.732068 21.795471 21.858690 21.921727 21.984583\n[176] 22.047260 22.109759 22.172082 22.234231 22.296206 22.358009 22.419642\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Apple stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\n\nMicrosoft\n\nDifferentiated Time Series Plot\n\nACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/microsoft_raw_data.csv\")\n#convert to time series data\nmyts<-ts(df$MSFT.Adjusted,frequency=365,start=c(2015,1,1))  \n#First order differentiation\ndf1 <- diff(myts)\n#ACF plot \nggAcf(df1,60,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -13.368, Lag order = 12, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are\nd = 1 p = 1 (PACF Plot) q = 1 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\nd=1\ni=1\ntemp = data.frame()\nls=matrix(rep(NA,6*4),nrow=4) #nrow = 2x2x1\n\n\nfor (p in 1:2)# p=0,1 :2\n{\n  for(q in 1:2)# q=0,1 :2\n  {\n    for(d in 1)# d=1 :1\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(myts,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        \n      }\n      \n    }\n  }\n}\n\nmodel = as.data.frame(ls)\nnames(model)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#table\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n10498.65\n10509.90\n10498.65\n\n\n0\n1\n1\n10474.02\n10490.90\n10474.03\n\n\n1\n1\n0\n10474.76\n10491.64\n10474.77\n\n\n1\n1\n1\n10475.45\n10497.96\n10475.47\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,1) with drift \n\nCoefficients:\n          ma1   drift\n      -0.1156  0.1017\ns.e.   0.0223  0.0605\n\nsigma^2 = 9.626:  log likelihood = -5234.01\nAIC=10474.02   AICc=10474.03   BIC=10490.9\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value and BIC value corresponds to an ARIMA (0,1,1) model. Additionally, the auto.arima function in R suggests an ARIMA (0,1,1) model as the best fit for the data. Since the model parameter are the same, we can proceed with model diagnostic for the parameters.\n\n\nModel Diagnostic\n\nModel PlotModelResiduals\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 0,1,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[12:42], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ma1  constant\n      -0.1156    0.1017\ns.e.   0.0223    0.0605\n\nsigma^2 estimated as 9.617:  log likelihood = -5234.01,  aic = 10474.02\n\n$degrees_of_freedom\n[1] 2050\n\n$ttable\n         Estimate     SE t.value p.value\nma1       -0.1156 0.0223 -5.1907  0.0000\nconstant   0.1017 0.0605  1.6800  0.0931\n\n$AIC\n[1] 5.104299\n\n$AICc\n[1] 5.104302\n\n$BIC\n[1] 5.112525\n\n\n\n\n\n\nCode\narima <- auto.arima(myts)\ncheckresiduals(arima)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,1) with drift\nQ* = 619.68, df = 410, p-value = 9.262e-11\n\nModel df: 1.   Total lags used: 411\n\n\n\n\n\nThe best model is ARIMA(0,1,1). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1, approaching 6 in the recent years. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very few significant lags, which is a positive sign for the model. The qq-plot also suggests some normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[Y_t = Y_{t-1} + \\theta_1 (e_{t-1}) + e_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(0,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Microsoft Stock Prices Prediction\") +\n  ylab(\"stock prices\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,182, 0,1,1, main='Microsoft Stock Prices Prediction')\n\n\n\n\n\n$pred\nTime Series:\nStart = c(2020, 229) \nEnd = c(2021, 45) \nFrequency = 365 \n  [1] 249.6160 249.7177 249.8195 249.9212 250.0229 250.1246 250.2264 250.3281\n  [9] 250.4298 250.5315 250.6333 250.7350 250.8367 250.9384 251.0402 251.1419\n [17] 251.2436 251.3453 251.4471 251.5488 251.6505 251.7522 251.8540 251.9557\n [25] 252.0574 252.1591 252.2608 252.3626 252.4643 252.5660 252.6677 252.7695\n [33] 252.8712 252.9729 253.0746 253.1764 253.2781 253.3798 253.4815 253.5833\n [41] 253.6850 253.7867 253.8884 253.9902 254.0919 254.1936 254.2953 254.3971\n [49] 254.4988 254.6005 254.7022 254.8039 254.9057 255.0074 255.1091 255.2108\n [57] 255.3126 255.4143 255.5160 255.6177 255.7195 255.8212 255.9229 256.0246\n [65] 256.1264 256.2281 256.3298 256.4315 256.5333 256.6350 256.7367 256.8384\n [73] 256.9402 257.0419 257.1436 257.2453 257.3471 257.4488 257.5505 257.6522\n [81] 257.7539 257.8557 257.9574 258.0591 258.1608 258.2626 258.3643 258.4660\n [89] 258.5677 258.6695 258.7712 258.8729 258.9746 259.0764 259.1781 259.2798\n [97] 259.3815 259.4833 259.5850 259.6867 259.7884 259.8902 259.9919 260.0936\n[105] 260.1953 260.2970 260.3988 260.5005 260.6022 260.7039 260.8057 260.9074\n[113] 261.0091 261.1108 261.2126 261.3143 261.4160 261.5177 261.6195 261.7212\n[121] 261.8229 261.9246 262.0264 262.1281 262.2298 262.3315 262.4333 262.5350\n[129] 262.6367 262.7384 262.8401 262.9419 263.0436 263.1453 263.2470 263.3488\n[137] 263.4505 263.5522 263.6539 263.7557 263.8574 263.9591 264.0608 264.1626\n[145] 264.2643 264.3660 264.4677 264.5695 264.6712 264.7729 264.8746 264.9764\n[153] 265.0781 265.1798 265.2815 265.3832 265.4850 265.5867 265.6884 265.7901\n[161] 265.8919 265.9936 266.0953 266.1970 266.2988 266.4005 266.5022 266.6039\n[169] 266.7057 266.8074 266.9091 267.0108 267.1126 267.2143 267.3160 267.4177\n[177] 267.5195 267.6212 267.7229 267.8246 267.9264 268.0281\n\n$se\nTime Series:\nStart = c(2020, 229) \nEnd = c(2021, 45) \nFrequency = 365 \n  [1]  3.101066  4.139869  4.965926  5.672947  6.301130  6.872128  7.399193\n  [8]  7.891132  8.354153  8.792825  9.210629  9.610286  9.993973 10.363465\n [15] 10.720229 11.065497 11.400313 11.725572 12.042049 12.350419 12.651275\n [22] 12.945141 13.232482 13.513714 13.789213 14.059313 14.324322 14.584516\n [29] 14.840149 15.091452 15.338639 15.581904 15.821430 16.057383 16.289919\n [36] 16.519182 16.745306 16.968417 17.188633 17.406062 17.620809 17.832970\n [43] 18.042636 18.249894 18.454824 18.657504 18.858005 19.056397 19.252744\n [50] 19.447110 19.639551 19.830126 20.018886 20.205883 20.391165 20.574779\n [57] 20.756768 20.937176 21.116042 21.293406 21.469305 21.643774 21.816848\n [64] 21.988560 22.158941 22.328023 22.495833 22.662401 22.827753 22.991916\n [71] 23.154916 23.316776 23.477520 23.637171 23.795751 23.953281 24.109782\n [78] 24.265273 24.419775 24.573305 24.725881 24.877522 25.028244 25.178064\n [85] 25.326998 25.475061 25.622269 25.768635 25.914175 26.058902 26.202830\n [92] 26.345971 26.488339 26.629946 26.770803 26.910924 27.050318 27.188998\n [99] 27.326975 27.464258 27.600858 27.736785 27.872050 28.006661 28.140629\n[106] 28.273961 28.406668 28.538758 28.670239 28.801120 28.931409 29.061114\n[113] 29.190242 29.318802 29.446800 29.574245 29.701143 29.827500 29.953325\n[120] 30.078624 30.203402 30.327667 30.451426 30.574683 30.697445 30.819718\n[127] 30.941508 31.062821 31.183662 31.304036 31.423949 31.543406 31.662413\n[134] 31.780974 31.899094 32.016779 32.134032 32.250859 32.367265 32.483253\n[141] 32.598829 32.713996 32.828760 32.943123 33.057091 33.170668 33.283856\n[148] 33.396662 33.509087 33.621136 33.732814 33.844123 33.955066 34.065649\n[155] 34.175874 34.285744 34.395264 34.504436 34.613263 34.721750 34.829898\n[162] 34.937712 35.045194 35.152347 35.259175 35.365680 35.471866 35.577734\n[169] 35.683288 35.788531 35.893466 35.998094 36.102419 36.206444 36.310171\n[176] 36.413602 36.516740 36.619588 36.722148 36.824422 36.926413 37.028122\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Microsoft stock price. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\n\nGross Domestic Product Growth Rate\n\nStationary Time Series\n\nACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/gdp_clean_data.csv\")\n#convert to time series data\nmyts<-ts(df$value,frequency=4,start=c(2010/1/1))\n#ACF plot \nggAcf(myts,main=\"ACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(myts,main=\"PACF Plot\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(myts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  myts\nDickey-Fuller = -5.768, Lag order = 3, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggAcf(myts,lag = 4,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 4)\nggPacf(myts,lag = 4,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nUpon analyzing the ACF and PACF plots, it was observed that most of the bar lines lie between the blue lines, indicating stationarity of the time series. This has been further confirmed by the Augmented Dickey-Fuller test, as evidenced by a p-value of less than 0.05. Given that the data is already stationary but there is presence of seasonality, an SARIMA model is preferred over ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p and q in the ARMA model. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 0 D = 0 p = 0,1 (PACF Plot) q = 0,1 (ACF Plot) P = 0,1 (PACF Seasonality Plot) Q = 0,1 (ACF Seasonality Plot)\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=0\n  D=0\n  s=4\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*16),nrow=16)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,1,; Q=0,1 and PACF plot: p=0,1; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,p2=2,q1=1,q2=2,P1=1,P2=2,Q1=1,Q2=2,data=myts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n0\n0\n0\n241.5679\n245.4703\n241.8128\n\n\n0\n0\n0\n0\n0\n1\n207.2709\n213.1247\n207.7709\n\n\n0\n0\n0\n1\n0\n0\n228.4538\n234.3075\n228.9538\n\n\n0\n0\n0\n1\n0\n1\n207.6683\n215.4733\n208.5194\n\n\n0\n0\n1\n0\n0\n0\n235.1650\n241.0187\n235.6650\n\n\n0\n0\n1\n0\n0\n1\n201.5710\n209.3760\n202.4221\n\n\n0\n0\n1\n1\n0\n0\n221.6803\n229.4853\n222.5314\n\n\n0\n0\n1\n1\n0\n1\n202.2159\n211.9721\n203.5202\n\n\n1\n0\n0\n0\n0\n0\n234.0196\n239.8734\n234.5196\n\n\n1\n0\n0\n0\n0\n1\n199.8027\n207.6077\n200.6538\n\n\n1\n0\n0\n1\n0\n0\n219.0909\n226.8959\n219.9419\n\n\n1\n0\n0\n1\n0\n1\n200.3070\n210.0632\n201.6113\n\n\n1\n0\n1\n0\n0\n0\n236.0128\n243.8178\n236.8639\n\n\n1\n0\n1\n0\n0\n1\n201.7720\n211.5282\n203.0763\n\n\n1\n0\n1\n1\n0\n0\n221.0125\n230.7687\n222.3169\n\n\n1\n0\n1\n1\n0\n1\n202.2245\n213.9320\n204.0912\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(2,0,1)[4] with non-zero mean \n\nCoefficients:\n         ar1     sar1     sar2     sma1    mean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 = 2.105:  log likelihood = -94.11\nAIC=200.22   AICc=202.09   BIC=211.93\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,0,0) and seasonal parameters are the least (0,0,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and seasonal parameters (2,0,1).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[26:57], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1  constant\n      -0.3051  -1.0000    0.0012\ns.e.   0.1329   0.1524    0.0417\n\nsigma^2 estimated as 2.492:  log likelihood = -100.93,  aic = 209.87\n\n$degrees_of_freedom\n[1] 48\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.3051 0.1329 -2.2954  0.0261\nsma1      -1.0000 0.1524 -6.5608  0.0000\nconstant   0.0012 0.0417  0.0290  0.9770\n\n$AIC\n[1] 4.115092\n\n$AICc\n[1] 4.125105\n\n$BIC\n[1] 4.266608\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,2,0,1,4))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[38:70], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     sar1     sar2     sma1   xmean\n      0.4006  -0.3997  -0.3431  -0.7034  2.0673\ns.e.  0.1292   0.2515   0.2512   0.2626  0.0741\n\nsigma^2 estimated as 1.903:  log likelihood = -94.11,  aic = 200.22\n\n$degrees_of_freedom\n[1] 47\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.4006 0.1292  3.1000  0.0033\nsar1   -0.3997 0.2515 -1.5893  0.1187\nsar2   -0.3431 0.2512 -1.3658  0.1785\nsma1   -0.7034 0.2626 -2.6781  0.0102\nxmean   2.0673 0.0741 27.9061  0.0000\n\n$AIC\n[1] 3.850466\n\n$AICc\n[1] 3.87555\n\n$BIC\n[1] 4.07561\n\n\n\n\n\nIn the first model, the p-values for both ar1 and sma1 are less than 0.05, indicating that both coefficients are statistically significant. In the second model, the p-value for ar1 is less than 0.05, indicating that it is statistically significant, while the p-values for the seasonal coefficients (sar1 and sar2) are greater than 0.05, indicating that they are not statistically significant. The p-value for sma1 in the second model is less than 0.05, indicating that it is statistically significant. Based on the p-values, it appears that the first model (SARIMA(1,1,0)(0,1,1)[4]) has a statistically significant AR and MA component, while the second model (SARIMA(1,1,1)(0,0,2)[4]) has a statistically significant.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[4]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^4)(1-\\Phi_1B^4)y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0),seasonal = c(0,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"GDP Growth Rate Prediction\") +\n  ylab(\"GDP growth\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,12, 1,1,0,0,1,1,4, main='GDP Growth Rate Prediction')\n\n\n\n\n\n$pred\n          Qtr1      Qtr2      Qtr3      Qtr4\n2023 1.0795445 1.0756510 1.0580210 1.0348134\n2024 0.9436877 1.0147972 0.9763838 0.9589353\n2025 0.8662137 0.9377654 0.8992295 0.8818150\n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023 2.699600 3.327957 3.963720 4.483195\n2024 5.048923 5.523866 5.967367 6.377804\n2025 6.838954 7.240686 7.626250 7.991102\n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted GDP Growth Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\n\nInterest Rate\n\nDifferentiated Time Series Plot\n\nACFPACFADF TestSeasonal ACF PlotSeasonal PACF Ploy\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/interest_rate_clean_data.csv\")\n#convert to ts data\nmyts<-ts(df$value,frequency=12,start=c(2010/1/1))\n#First order differentiation\ndf1 <- diff(myts)\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,60,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -4.1249, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\ndf1_seasonal = myts %>%diff(differences = 1, lag = 12)\nggAcf(df1_seasonal,main=\"ACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\n\nCode\n# Seasonal differencing (period = 12)\nggPacf(df1_seasonal,main=\"PACF Plot: Seasonality\")\n\n\n\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary and there is presence of seasonality, the next step is to model it using SARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. P and Q are determined similarly, from seasonal plot.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot) P = 1 (PACF Seasonality Plot) Q = 1,2,3,4 (ACF Seasonality Plot) D = 1\n\n\nModel Selection\n\nSARIMA ResultAuto Arima Model\n\n\n\n\nCode\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,q1,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*7),nrow=7)\n  \n  \n  for (p in p1)\n  {\n    for(q in q1)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n\n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n# q=0,; Q=0,1,2,3,4 and PACF plot: p=0; P=0,1, D=0 and d=O\noutput=SARIMA.c(p1=1,q1=1,P1=1,P2=2,Q1=1,Q2=5,data=myts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-38.60554\n-35.62193\n-38.57776\n\n\n0\n1\n0\n0\n1\n1\n-61.11557\n-55.14836\n-61.03165\n\n\n0\n1\n0\n0\n1\n2\n-59.78470\n-50.83388\n-59.61569\n\n\n0\n1\n0\n0\n1\n3\n-60.70802\n-48.77359\n-60.42433\n\n\n0\n1\n0\n1\n1\n0\n-57.49539\n-51.52817\n-57.41147\n\n\n0\n1\n0\n1\n1\n1\n-60.79010\n-51.83928\n-60.62109\n\n\n0\n1\n0\n1\n1\n2\n-60.05246\n-48.11803\n-59.76877\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(1,0,0)(1,0,0)[12] with non-zero mean \n\nCoefficients:\n         ar1    sar1    mean\n      0.9548  0.2623  0.8646\ns.e.  0.0267  0.0940  0.3954\n\nsigma^2 = 0.03157:  log likelihood = 48.82\nAIC=-89.64   AICc=-89.38   BIC=-77.36\n\n\n\n\n\nSorting the table reveals that the AIC and BIC with the parameters (1,1,0) and seasonal parameters are the least (0,1,1). There is also a function that enables the automatic selection of an ARIMA model. According to R’s auto.arima technique, the model’s parameters should be (1,0,0) and (1,0,0).\nSince there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,1,0,0,1,1,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[18:48], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     sma1\n      -0.0786  -0.5590\ns.e.   0.0869   0.1293\n\nsigma^2 estimated as 0.03614:  log likelihood = 32.97,  aic = -59.93\n\n$degrees_of_freedom\n[1] 144\n\n$ttable\n     Estimate     SE t.value p.value\nar1   -0.0786 0.0869 -0.9045  0.3673\nsma1  -0.5590 0.1293 -4.3227  0.0000\n\n$AIC\n[1] -0.4104909\n\n$AICc\n[1] -0.4099161\n\n$BIC\n[1] -0.3491839\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,0,1,0,0,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[60:91], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1    sar1   xmean\n      0.9548  0.2623  0.8646\ns.e.  0.0267  0.0940  0.3954\n\nsigma^2 estimated as 0.03097:  log likelihood = 48.82,  aic = -89.64\n\n$degrees_of_freedom\n[1] 156\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.9548 0.0267 35.7690  0.0000\nsar1    0.2623 0.0940  2.7898  0.0059\nxmean   0.8646 0.3954  2.1866  0.0303\n\n$AIC\n[1] -0.5637746\n\n$AICc\n[1] -0.5628008\n\n$BIC\n[1] -0.4865695\n\n\n\n\n\nLooking at the p-values of the coefficients in the two models, we can see that all the coefficients in the second model have p-values less than 0.05, indicating that they are statistically significant at the 5% level. In contrast, the p-value of the AR(1) coefficient in the first model is 0.3673, indicating that it is not statistically significant at the 5% level.Therefore, based on the p-values alone, the first model with SARIMA(1,1,0)(0,1,1)[12] appears to be the better model.\nBy analyzing the standardized residuals plot for the model (SARIMA(1,1,0)(0,1,1)[12]), it can be observed that the mean is close to 0 and the variance is higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[ (1-\\phi_1B)(1-B^{12})(1-\\Phi_1B^{12}))y_t = \\epsilon_t \\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,1,0), seasonal = c(0,1,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Interest Rate Prediction\") +\n  ylab(\"Interest Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 1,1,0,0,1,1,12, main = \"Interest Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                            2.282059 2.409382 2.295342 2.289331 2.117888\n2024 2.704908 2.565338 2.802065 3.054416 3.179239 3.065396 3.059369 2.887927\n2025 3.474948 3.335377 3.572105 3.824456 3.949279 3.835436 3.829409 3.657967\n2026 4.244988 4.105417 4.342144                                             \n          Sep      Oct      Nov      Dec\n2023 2.273762 2.634956 2.731161 2.557040\n2024 3.043801 3.404995 3.501201 3.327079\n2025 3.813841 4.175035 4.271240 4.097119\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                               0.1901105 0.2585052 0.3129230 0.3591436\n2024 0.5615857 0.5885951 0.6144183 0.6672023 0.7137387 0.7575948 0.7990346\n2025 1.0126712 1.0440358 1.0744853 1.1263407 1.1740351 1.2200070 1.2642976\n2026 1.5028805 1.5390527 1.5743941                                        \n           Aug       Sep       Oct       Nov       Dec\n2023 0.4000625 0.4371678 0.4713612 0.5032366 0.5332099\n2024 0.8384296 0.8760547 0.9121292 0.9468302 0.9803037\n2025 1.3070892 1.3485235 1.3887222 1.4277895 1.4658159\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Interest Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,1,0),seasonal = c(0,1,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\n\nInflation Rate\n\nDifferentiated Time Series Plot\n\nACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/inflation_rate_clean_data.csv\")\n#convert the data to time series data\nmyts <- ts(as.vector(t(as.matrix(df))), start=c(2010,1), end=c(2023,2), frequency=12)\n#First order differentiation\ndf1 <- diff(myts)\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,36,main=\"PACF Plot: First order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -3.8576, Lag order = 5, p-value = 0.01802\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 1,3 (PACF Plot) q = 1 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\n#Combinations\narima_1 <- arima(myts, order = c(1,1,1)) # ARMA(1,1,1)\narima_2 <- arima(myts, order = c(3,1,1)) # ARMA(3,1,1)\n\n\n# Obtain AIC for all models \nAIC <- c(AIC(arima_1), AIC(arima_2)) \n\n\n# Obtain BIC for all models \nBIC <- c(BIC(arima_1), BIC(arima_2)) \n\np <- c(1,3)\nd <- c(1,1)\nq <- c(1,1)\n\nmodel = cbind(p,d,q,AIC,BIC)\n#table\nknitr::kable(model)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\n\n\n\n\n1\n1\n1\n-75.90608\n-66.73734\n\n\n3\n1\n1\n-84.47075\n-69.18952\n\n\n\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(4,1,1)(0,0,1)[12] with drift \n\nCoefficients:\n         ar1      ar2      ar3     ar4      ma1     sma1   drift\n      1.1757  -0.2352  -0.3924  0.3603  -0.6966  -0.5922  0.0215\ns.e.  0.1166   0.1251   0.1162  0.0757   0.1037   0.0807  0.0173\n\nsigma^2 = 0.02376:  log likelihood = 71.61\nAIC=-127.23   AICc=-126.25   BIC=-102.78\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value and BIC value corresponds to an ARIMA (3,1,1) model. Additionally, the auto.arima function in R suggests an ARIMA parameters (4,1,1) and seasonal parameters are the least (0,0,1) as the best fit model. Since there are different models to choose from, it is important to perform model diagnostics to determine the best model for the data. These diagnostics can include checking for residual normality, checking for autocorrelation and partial autocorrelation in the residuals, and comparing the forecasted values to the actual values. By conducting these tests, we can determine which model provides the best fit for the data and is therefore the most appropriate to use for forecasting future values.\n\n\nModel Diagnostic\n\nModel 1 PlotModel 1Model 2 PlotModel 2\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 3,1,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[25:58], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ar2      ar3     ma1  constant\n      0.3485  0.1198  -0.2922  0.2113    0.0254\ns.e.  0.2297  0.1452   0.0791  0.2360    0.0209\n\nsigma^2 estimated as 0.03167:  log likelihood = 47.94,  aic = -83.89\n\n$degrees_of_freedom\n[1] 152\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.3485 0.2297  1.5167  0.1314\nar2        0.1198 0.1452  0.8253  0.4105\nar3       -0.2922 0.0791 -3.6936  0.0003\nma1        0.2113 0.2360  0.8955  0.3719\nconstant   0.0254 0.0209  1.2138  0.2267\n\n$AIC\n[1] -0.5343063\n\n$AICc\n[1] -0.5317754\n\n$BIC\n[1] -0.4175071\n\n\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 4,1,1,0,0,1,12))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[46:81], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1      ar2      ar3     ar4      ma1     sma1  constant\n      1.1757  -0.2352  -0.3924  0.3603  -0.6966  -0.5922    0.0215\ns.e.  0.1166   0.1251   0.1162  0.0757   0.1037   0.0807    0.0173\n\nsigma^2 estimated as 0.0227:  log likelihood = 71.61,  aic = -127.23\n\n$degrees_of_freedom\n[1] 150\n\n$ttable\n         Estimate     SE t.value p.value\nar1        1.1757 0.1166 10.0868  0.0000\nar2       -0.2352 0.1251 -1.8803  0.0620\nar3       -0.3924 0.1162 -3.3768  0.0009\nar4        0.3603 0.0757  4.7621  0.0000\nma1       -0.6966 0.1037 -6.7199  0.0000\nsma1      -0.5922 0.0807 -7.3380  0.0000\nconstant   0.0215 0.0173  1.2380  0.2177\n\n$AIC\n[1] -0.8103599\n\n$AICc\n[1] -0.8055721\n\n$BIC\n[1] -0.6546276\n\n\n\n\n\nA smaller p-value indicates that a coefficient is more statistically significant. Looking at the output provided, the second model SARMIA (4,1,1)(0,0,1)[12] appears to have lower p-values for all of the coefficients, which suggests that it is a better model in terms of statistical significance.\nBy analyzing the standardized residuals plot for the model (SARIMA(4,1,1)(0,0,1)[12]), it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows very low significant lags, which is a high sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0.05, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[\\Delta Y_t = \\phi_1 \\Delta Y_{t-1} + \\phi_2 \\Delta Y_{t-2} + \\phi_3 \\Delta Y_{t-3} + \\phi_4 \\Delta Y_{t-4} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t + \\theta_{12} \\epsilon_{t-12}\\] #### Forcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(4,1,1),seasonal = c(0,0,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Inflation Rate Prediction\") +\n  ylab(\"Inflation Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 4,1,1,0,0,1,12, main = \"Inflation Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                   5.495776 5.389748 5.344802 5.205688 5.249631 5.164470\n2024 5.283954 5.283987 5.290514 5.314089 5.337350 5.358570 5.373114 5.386553\n2025 5.476534 5.494819 5.513071 5.531901 5.551331 5.571139 5.591010 5.610839\n2026 5.711529 5.732022                                                      \n          Sep      Oct      Nov      Dec\n2023 5.064247 5.138831 5.273497 5.297473\n2024 5.400955 5.418630 5.437955 5.457675\n2025 5.630673 5.650633 5.670779 5.691099\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                     0.1506640 0.2690050 0.3827598 0.4595446 0.5219644\n2024 0.9329297 1.0004244 1.0394805 1.0695190 1.0948899 1.1240493 1.1549882\n2025 1.3042817 1.3255537 1.3460871 1.3656904 1.3844121 1.4024993 1.4201440\n2026 1.5180451 1.5331652                                                  \n           Aug       Sep       Oct       Nov       Dec\n2023 0.5789069 0.6431488 0.7142951 0.7894332 0.8627722\n2024 1.1860724 1.2138042 1.2384755 1.2609465 1.2827317\n2025 1.4374323 1.4543438 1.4708448 1.4869326 1.5026483\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Inflation Rate. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(4,1,1),seasonal = c(0,0,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(4,1,1),seasonal = c(0,0,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\n\nUnemployment Rate\n\nDifferentiated Time Series Plot\n\nACFPACFADF Test\n\n\n\n\nCode\n#import the data\ndf <- read.csv(\"DATA/CLEANED DATA/unemployment_rate_clean_data.csv\")\n#convert the data to ts data\nmyts<-ts(df$Value,frequency=12,start=c(2010/1/1))\n#First order differentiation\ndf1 <- diff(myts)\n#ACF plot \nggAcf(df1,36,main=\"ACF Plot: Second order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\nggPacf(df1,36,main=\"PACF Plot: Second order differentiation\") \n\n\n\n\n\n\n\n\n\nCode\ntseries::adf.test(df1)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  df1\nDickey-Fuller = -6.457, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\nAfter analyzing the ACF and PACF plots, it can be observed that most of the bar lines lie between the blue lines, indicating that the time series is stationary. This has been further confirmed through an Augmented Dickey-Fuller test as the p-value is less than 0.05. Once the data is stationary, the next step is to model it using ARIMA. The combination of ACF and PACF plots can help determine the appropriate values for p, d, and q in the ARIMA model. The order of differencing required to achieve stationarity gives the d value. The p value is determined by examining significant spikes at various lags in the PACF plot, while the q value is determined through significant spikes in the ACF plot. It is important to consider other methods such as grid search and information criteria to ensure the most appropriate model is selected.\nHere the parameters are d = 1 p = 0 (PACF Plot) q = 0 (ACF Plot)\n\n\nModel Selection\n\nARIMA ResultAuto Arima Model\n\n\n\n\nCode\nmodel <- arima(myts,c(0,1,0))\nmodel\n\n\n\nCall:\narima(x = myts, order = c(0, 1, 0))\n\n\nsigma^2 estimated as 0.7845:  log likelihood = -203.72,  aic = 409.43\n\n\n\n\n\n\nCode\nauto.arima(myts)\n\n\nSeries: myts \nARIMA(0,1,0) \n\nsigma^2 = 0.7845:  log likelihood = -203.72\nAIC=409.43   AICc=409.46   BIC=412.49\n\n\n\n\n\nThe given table provides different ARIMA models with their corresponding AIC and BIC values. By sorting the table according to each criterion, we can identify the models with the lowest values. The smallest AIC value and BIC value corresponds to an ARIMA (0,1,0) model. Additionally, the auto.arima function in R suggests an ARIMA (0,1,0) model as the best fit for the data. Since the model parameter are the same, we can proceed with model diagnostic for the parameters.\n\n\nModel Diagnostic\n\nModel PlotModel 1Residual\n\n\n\n\nCode\nmodel_output <- capture.output(sarima(myts, 1,0,1))\n\n\n\n\n\n\n\n\n\nCode\ncat(model_output[62:93], model_output[length(model_output)], sep = \"\\n\") \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = xmean, include.mean = FALSE, transform.pars = trans, fixed = fixed, \n    optim.control = list(trace = trc, REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1     ma1   xmean\n      0.9156  0.0771  6.1460\ns.e.  0.0355  0.0950  0.8277\n\nsigma^2 estimated as 0.7549:  log likelihood = -202.96,  aic = 413.93\n\n$degrees_of_freedom\n[1] 155\n\n$ttable\n      Estimate     SE t.value p.value\nar1     0.9156 0.0355 25.7721  0.0000\nma1     0.0771 0.0950  0.8118  0.4182\nxmean   6.1460 0.8277  7.4254  0.0000\n\n$AIC\n[1] 2.619783\n\n$AICc\n[1] 2.620769\n\n$BIC\n[1] 2.697317\n\n\n\n\n\n\nCode\narima <- auto.arima(myts)\ncheckresiduals(arima)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,0)\nQ* = 7.2289, df = 24, p-value = 0.9996\n\nModel df: 0.   Total lags used: 24\n\n\n\n\n\nThe best model is ARIMA(1,0,1). By analyzing the standardized residuals plot, it can be observed that the mean is close to 0 and the variance is slightly higher than 1. Deviations from these values could indicate poor model fit. However, in this case, the model seems to be well-fitted. The ACF plot of residuals shows no significant lags, which is a positive sign for the model. The qq-plot also suggests high normality in the residuals. Additionally, the p-values for the Ljung-Box test are near than 0, indicating that the residuals are independent, which is favorable for the model’s accuracy.\nThe equation for this model: \\[\\Delta Y_t = \\phi_1 \\Delta Y_{t-1} + \\theta_1 \\epsilon_{t-1} + \\epsilon_t\\]\n\n\nForcast\n\nShort Term ForcastLong term Forcast\n\n\n\n\nCode\nmyts %>%\n  Arima(order=c(1,0,1),include.drift = TRUE) %>%\n  forecast %>%\n  autoplot(main = \"Unemployment Rate Prediction\") +\n  ylab(\"Unemployment Rate\") + xlab(\"Year\")\n\n\n\n\n\n\n\n\n\nCode\nsarima.for(myts,36, 1,0,1, main = \"Unemployment Rate Prediction\")\n\n\n\n\n\n$pred\n          Jan      Feb      Mar      Apr      May      Jun      Jul      Aug\n2023                   3.814151 4.010889 4.191028 4.355968 4.506993 4.645275\n2024 5.180167 5.261654 5.336266 5.404583 5.467136 5.524412 5.576855 5.624873\n2025 5.810614 5.838911 5.864819 5.888542 5.910264 5.930153 5.948364 5.965038\n2026 6.029536 6.039362                                                      \n          Sep      Oct      Nov      Dec\n2023 4.771891 4.887824 4.993976 5.091172\n2024 5.668841 5.709098 5.745959 5.779711\n2025 5.980306 5.994285 6.007085 6.018805\n2026                                    \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2023                     0.8688527 1.2243105 1.4569544 1.6265546 1.7561668\n2024 2.1374499 2.1671027 2.1916538 2.2120269 2.2289639 2.2430649 2.2548189\n2025 2.2941710 2.2975230 2.3003295 2.3026798 2.3046483 2.3062974 2.3076791\n2026 2.3123513 2.3127525                                                  \n           Aug       Sep       Oct       Nov       Dec\n2023 1.8578756 1.9390393 2.0045542 2.0578738 2.1015336\n2024 2.2646262 2.2728158 2.2796591 2.2853807 2.2901664\n2025 2.3088368 2.3098070 2.3106200 2.3113014 2.3118726\n2026                                                  \n\n\n\n\n\nThe two figures above show an idea of the short term and the long-term forecasted Microsoft. The forecasted values is be based on the estimated parameters of the model, which were obtained by fitting it to the historical data. One way to assess the accuracy of the forecasts is to use cross-validation, which involves splitting the data into training and testing sets and comparing the predicted values to the actual values in the test set. This can help to identify any potential problems with the model and to fine-tune the model parameters.\n\n12 Step ahead Cross Validation1 Step ahead Cross Validation\n\n\n\n\nCode\n#a seasonal cross validation using 12 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,0,1)), h=h)}\n\n# Compute cross-validated errors for up to 12 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 12)\n\n# Compute the MSE values and remove missing values\nmse <- colMeans(e^2, na.rm = TRUE)\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"12 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\n\nCode\n#a seasonal cross validation using 1 steps ahead forecasts\nfarima1 <- function(x, h){forecast(Arima(x, order=c(1,0,1)),h=h)}\n\n# Compute cross-validated errors for up to 1 steps ahead\ne <- tsCV(myts, forecastfunction = farima1, h = 1)\nmse <-abs(mean(e,na.rm=TRUE))\n\n# Plot the MSE values against the forecast horizon\ndata.frame(h = 1:12, MSE = mse) %>%\n  ggplot(aes(x = h, y = MSE)) + geom_point()+geom_line()+ggtitle(\"1 Step Ahead Cross Validation\")\n\n\n\n\n\n\n\n\nThe one-step-ahead forecasting plot shows that the MSE seems to be stable with only a small rise. The MSE goes up by 12 steps, and each step is cross-validated. Lastly, the above-mentioned ARIMA model should be compared to benchmark methods before being used for a long time.\n\n\nModel Comparison\n\nPlotModel Error Table\n\n\n\n\nCode\nfit <- Arima(myts, order=c(1,0,1))\nautoplot(myts) +\n  autolayer(meanf(myts, h=56),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(myts, h=56),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(myts, h=56),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(myts, h=56, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,56), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\n\n\nCode\nsummary <- summary(fit)\nsnaive <- snaive(myts, h=36)\nError <- c(\"ME\",\"RMSE\",\"MAE\",\"MPE\",\"MAPE\",\"MASE\",\"ACF1\")\nModel <- c(0.1991357, 4.715155, 2.942283, 0.05926238, 1.116543, 0.03522657, -0.00659621)\nSnavie <- c(82.81296, 98.39218, 83.52453, 27.23555, 27.57219, 1, 0.9922749)\ntable <- data.frame(Error,Model,Snavie)  \n\n\n\n\n\nError\nModel\nSnavie\n\n\n\n\nME\n0.19913570\n82.81296\n\n\nRMSE\n4.71515500\n98.392180\n\n\nMAE\n2.94228300\n83.52453\n\n\nMPE\n0.05926238\n27.23555\n\n\nMAPE\n1.11654300\n27.57219\n\n\nMASE\n0.03522657\n1.0000000\n\n\nACF1\n-0.00659621\n0.9922749"
  }
]